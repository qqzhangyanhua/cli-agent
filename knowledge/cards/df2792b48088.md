# 执行工作流

- 类型: heading
- 来源: /Users/zhangyanhua/Desktop/AI/tushare/quantification/example/docs/STREAMING_OUTPUT.md

## 摘要
# 流式输出优化文档 ## 问题分析 ### 用户痛点 用户在使用 AI 智能终端助手时，提出问题后需要等待 LLM 完整响应（通常 5-10 秒），期间没有任何反馈，**感觉卡卡的**。 ### 根本原因 - **感知性能问题**：不是真实性能慢，而是用户感知到的等待时间长 - **缺少反馈**：等待期间黑屏，用户不知道系统是否在工作 - **心理焦虑**：看不到进度，担心程序卡死 ### Linus 式分析 **数据流问题**： ``` Before（旧架构）: 用户输入 → LLM生成 → [等待5-10秒] → 一次性显示全部 ↑ ↑ 开始计算 用户焦虑 After（新架构）: 用户输入 → LLM生成 → [逐Token流式输出] → 实时显示 ↑ ↑ 开始计算 立即有反馈，用户安心 ``` **核心洞察**： > "这不是技术问题，是用户体验问题。解决方案很简单：流式输出。" 
