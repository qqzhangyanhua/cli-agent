"""
å·¥ä½œæµèŠ‚ç‚¹æ¨¡å—
åŒ…å«æ‰€æœ‰LangGraphèŠ‚ç‚¹å‡½æ•°
"""

import json
from langchain_core.messages import HumanMessage
from agent_config import AgentState, LLM_CONFIG, LLM_CONFIG2
from agent_memory import memory
from agent_utils import execute_terminal_command
from agent_llm import llm, llm_code
from mcp_manager import mcp_manager


# ============================================
# æ„å›¾åˆ†æå’Œè§„åˆ’èŠ‚ç‚¹
# ============================================

def intent_analyzer(state: AgentState) -> dict:
    """åˆ†æç”¨æˆ·æ„å›¾ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰"""
    user_input = state["user_input"]
    context = memory.get_context_string()

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ç»ˆç«¯åŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·è¾“å…¥å’Œå¯¹è¯å†å²ï¼Œåˆ†æç”¨æˆ·æ„å›¾ã€‚

{context}

å½“å‰ç”¨æˆ·è¾“å…¥: {user_input}

åˆ¤æ–­è§„åˆ™:
- å¦‚æœç”¨æˆ·æƒ³è¯»å–æ–‡ä»¶ã€å†™å…¥æ–‡ä»¶ã€åˆ—å‡ºç›®å½•ã€æœç´¢æ–‡ä»¶ã€è·å–æ–‡ä»¶ä¿¡æ¯ -> mcp_tool_call
- å¦‚æœç”¨æˆ·æƒ³æˆªå›¾ã€æ“ä½œå‰ªè´´æ¿ã€æ‰§è¡Œæ¡Œé¢å‘½ä»¤ -> mcp_tool_call
- å¦‚æœç”¨æˆ·æƒ³æ‰§è¡Œç³»ç»Ÿå‘½ä»¤ã€è¿è¡Œç¨‹åº -> terminal_command
- å¦‚æœç”¨æˆ·éœ€è¦åˆ›å»ºä»£ç æ–‡ä»¶å¹¶æ‰§è¡Œã€æˆ–è€…éœ€è¦å¤šä¸ªæ­¥éª¤å®Œæˆä»»åŠ¡ -> multi_step_command
- å¦‚æœç”¨æˆ·åœ¨é—®é—®é¢˜ã€å¯»æ±‚è§£é‡Šã€éœ€è¦å»ºè®®ã€æˆ–è€…å¼•ç”¨ä¹‹å‰çš„å¯¹è¯ -> question

åªè¿”å›ä¸€ä¸ªè¯: 'mcp_tool_call', 'terminal_command', 'multi_step_command' æˆ– 'question'

æ„å›¾:"""

    result = llm.invoke([HumanMessage(content=prompt)])
    intent = result.content.strip().lower()

    if intent not in ["mcp_tool_call", "terminal_command", "multi_step_command", "question"]:
        intent = "question"

    print(f"\n[æ„å›¾åˆ†æ] {user_input[:50]}...")
    print(f"           ä½¿ç”¨æ¨¡å‹: {LLM_CONFIG['model']}")
    print(f"           æ„å›¾: {intent}")

    return {"intent": intent}


def command_generator(state: AgentState) -> dict:
    """ç”Ÿæˆç»ˆç«¯å‘½ä»¤"""
    user_input = state["user_input"]
    recent_commands = memory.get_recent_commands()

    prompt = f"""å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€è¯·æ±‚è½¬æ¢ä¸ºç»ˆç«¯å‘½ä»¤ã€‚

{recent_commands}

å½“å‰è¯·æ±‚: {user_input}

ç¤ºä¾‹:
- "åˆ—å‡ºå½“å‰ç›®å½•çš„æ‰€æœ‰æ–‡ä»¶" -> ls -la
- "æŸ¥çœ‹Pythonç‰ˆæœ¬" -> python3 --version
- "æ˜¾ç¤ºå½“å‰è·¯å¾„" -> pwd

åªè¿”å›å‘½ä»¤æœ¬èº«ï¼Œä¸è¦è§£é‡Š:"""

    result = llm_code.invoke([HumanMessage(content=prompt)])
    command = result.content.strip()

    print(f"[å‘½ä»¤ç”Ÿæˆ] {command}")
    print(f"           ä½¿ç”¨æ¨¡å‹: {LLM_CONFIG2['model']}")

    return {"command": command}


def multi_step_planner(state: AgentState) -> dict:
    """å¤šæ­¥éª¤è§„åˆ’"""
    user_input = state["user_input"]
    recent_commands = memory.get_recent_commands()

    prompt = f"""åˆ†æç”¨æˆ·è¯·æ±‚ï¼Œè¿”å›JSONæ ¼å¼çš„æ‰§è¡Œè®¡åˆ’ã€‚

{recent_commands}

ç”¨æˆ·è¯·æ±‚: {user_input}

è¿”å›JSONå¯¹è±¡:
{{
  "needs_file_creation": true/false,
  "file_path": "æ–‡ä»¶è·¯å¾„",
  "file_content": "æ–‡ä»¶å†…å®¹",
  "commands": ["å‘½ä»¤1", "å‘½ä»¤2"]
}}

åªè¿”å›JSON:"""

    result = llm_code.invoke([HumanMessage(content=prompt)])
    plan_text = result.content.strip()
    
    if "```json" in plan_text:
        plan_text = plan_text.split("```json")[1].split("```")[0].strip()
    elif "```" in plan_text:
        plan_text = plan_text.split("```")[1].split("```")[0].strip()
    
    try:
        plan = json.loads(plan_text)
        print(f"[å¤šæ­¥éª¤è§„åˆ’] ä½¿ç”¨æ¨¡å‹: {LLM_CONFIG2['model']}")
        print(f"            éœ€è¦åˆ›å»ºæ–‡ä»¶: {plan.get('needs_file_creation', False)}")
        print(f"            å‘½ä»¤æ•°é‡: {len(plan.get('commands', []))}")
        
        return {
            "needs_file_creation": plan.get("needs_file_creation", False),
            "file_path": plan.get("file_path", ""),
            "file_content": plan.get("file_content", ""),
            "commands": plan.get("commands", [])
        }
    except json.JSONDecodeError:
        print(f"[å¤šæ­¥éª¤è§„åˆ’] JSONè§£æå¤±è´¥")
        return {
            "needs_file_creation": False,
            "file_path": "",
            "file_content": "",
            "commands": [],
            "error": "æ— æ³•è§£ææ‰§è¡Œè®¡åˆ’"
        }


def mcp_tool_planner(state: AgentState) -> dict:
    """è§„åˆ’MCPå·¥å…·è°ƒç”¨"""
    user_input = state["user_input"]
    
    available_tools = mcp_manager.list_available_tools()
    tools_desc = "\n".join([
        f"- {t['name']}: {t['description']} (å‚æ•°: {', '.join(t['params'])})"
        for t in available_tools
    ])
    
    prompt = f"""åˆ†æç”¨æˆ·è¯·æ±‚ï¼Œé€‰æ‹©åˆé€‚çš„MCPå·¥å…·å¹¶è¿”å›JSONæ ¼å¼ã€‚

å¯ç”¨å·¥å…·:
{tools_desc}

ç”¨æˆ·è¯·æ±‚: {user_input}

è¿”å›JSONæ ¼å¼:
{{
  "tool": "å·¥å…·åç§°",
  "params": {{å‚æ•°å: å‚æ•°å€¼}}
}}

ç¤ºä¾‹:
è¾“å…¥: "è¯»å–README.mdæ–‡ä»¶"
è¾“å‡º: {{
  "tool": "fs_read",
  "params": {{"file_path": "README.md"}}
}}

åªè¿”å›JSON:"""
    
    result = llm_code.invoke([HumanMessage(content=prompt)])
    plan_text = result.content.strip()
    
    if "```json" in plan_text:
        plan_text = plan_text.split("```json")[1].split("```")[0].strip()
    elif "```" in plan_text:
        plan_text = plan_text.split("```")[1].split("```")[0].strip()
    
    try:
        plan = json.loads(plan_text)
        print(f"[MCPå·¥å…·è§„åˆ’] ä½¿ç”¨æ¨¡å‹: {LLM_CONFIG2['model']}")
        print(f"            å·¥å…·: {plan.get('tool', 'unknown')}")
        print(f"            å‚æ•°: {plan.get('params', {})}")
        
        return {
            "mcp_tool": plan.get("tool", ""),
            "mcp_params": plan.get("params", {})
        }
    except json.JSONDecodeError as e:
        print(f"[MCPå·¥å…·è§„åˆ’] JSONè§£æå¤±è´¥: {e}")
        return {
            "mcp_tool": "",
            "mcp_params": {},
            "error": "æ— æ³•è§£æMCPå·¥å…·è§„åˆ’"
        }


def question_answerer(state: AgentState) -> dict:
    """å›ç­”ç”¨æˆ·é—®é¢˜"""
    user_input = state["user_input"]
    context = memory.get_context_string()
    recent_commands = memory.get_recent_commands()

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIç»ˆç«¯åŠ©æ‰‹ã€‚å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¯¹è¯å†å²æä¾›æ›´å¥½çš„å¸®åŠ©ã€‚

{context}

{recent_commands}

å½“å‰é—®é¢˜: {user_input}

è¯·ç®€æ´ä½†å…¨é¢åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœç”¨æˆ·æåˆ°"åˆšæ‰"ã€"ä¹‹å‰"ç­‰è¯ï¼Œè¯·å‚è€ƒå¯¹è¯å†å²ã€‚

å›ç­”:"""

    result = llm.invoke([HumanMessage(content=prompt)])
    response = result.content

    print(f"[é—®é¢˜å›ç­”] ç”Ÿæˆå›ç­”")
    print(f"           ä½¿ç”¨æ¨¡å‹: {LLM_CONFIG['model']}")

    return {"response": response}


# ============================================
# æ‰§è¡ŒèŠ‚ç‚¹
# ============================================

def file_creator(state: AgentState) -> dict:
    """åˆ›å»ºæ–‡ä»¶"""
    file_path = state["file_path"]
    file_content = state["file_content"]
    
    print(f"[æ–‡ä»¶åˆ›å»º] åˆ›å»ºæ–‡ä»¶: {file_path}")
    
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(file_content)
        print(f"[æ–‡ä»¶åˆ›å»º] âœ… æˆåŠŸåˆ›å»ºæ–‡ä»¶: {file_path}")
        return {"error": ""}
    except Exception as e:
        error_msg = f"æ–‡ä»¶åˆ›å»ºå¤±è´¥: {str(e)}"
        print(f"[æ–‡ä»¶åˆ›å»º] âŒ {error_msg}")
        return {"error": error_msg}


def command_executor(state: AgentState) -> dict:
    """æ‰§è¡Œå•ä¸ªç»ˆç«¯å‘½ä»¤"""
    command = state["command"]
    print(f"[æ‰§è¡Œå‘½ä»¤] {command}")

    result = execute_terminal_command(command)

    if result["success"]:
        print(f"[æ‰§è¡ŒæˆåŠŸ] è¾“å‡ºé•¿åº¦: {len(result['output'])} å­—ç¬¦")
        return {
            "command_output": result["output"],
            "error": ""
        }
    else:
        print(f"[æ‰§è¡Œå¤±è´¥] {result['error']}")
        return {
            "command_output": "",
            "error": result["error"]
        }


def multi_command_executor(state: AgentState) -> dict:
    """æ‰§è¡Œå¤šä¸ªç»ˆç«¯å‘½ä»¤"""
    commands = state["commands"]
    outputs = []
    
    print(f"[å¤šå‘½ä»¤æ‰§è¡Œ] å…±{len(commands)}ä¸ªå‘½ä»¤")
    
    for idx, command in enumerate(commands, 1):
        print(f"[å¤šå‘½ä»¤æ‰§è¡Œ] æ‰§è¡Œç¬¬{idx}ä¸ªå‘½ä»¤: {command}")
        result = execute_terminal_command(command)
        
        outputs.append({
            "command": command,
            "success": result["success"],
            "output": result["output"],
            "error": result["error"]
        })
        
        if result["success"]:
            print(f"[å¤šå‘½ä»¤æ‰§è¡Œ] âœ… ç¬¬{idx}ä¸ªå‘½ä»¤æ‰§è¡ŒæˆåŠŸ")
        else:
            print(f"[å¤šå‘½ä»¤æ‰§è¡Œ] âŒ ç¬¬{idx}ä¸ªå‘½ä»¤æ‰§è¡Œå¤±è´¥: {result['error']}")
    
    return {"command_outputs": outputs}


def mcp_tool_executor(state: AgentState) -> dict:
    """æ‰§è¡ŒMCPå·¥å…·"""
    tool_name = state["mcp_tool"]
    params = state["mcp_params"]
    
    print(f"[MCPå·¥å…·æ‰§è¡Œ] å·¥å…·: {tool_name}")
    print(f"            å‚æ•°: {params}")
    
    try:
        result = mcp_manager.call_tool(tool_name, **params)
        
        if result.get("success"):
            print(f"[MCPå·¥å…·æ‰§è¡Œ] âœ… æˆåŠŸ")
        else:
            print(f"[MCPå·¥å…·æ‰§è¡Œ] âŒ å¤±è´¥: {result.get('error')}")
        
        return {"mcp_result": json.dumps(result, ensure_ascii=False)}
    
    except Exception as e:
        error_result = {"success": False, "error": str(e)}
        print(f"[MCPå·¥å…·æ‰§è¡Œ] âŒ å¼‚å¸¸: {e}")
        return {"mcp_result": json.dumps(error_result, ensure_ascii=False)}


# ============================================
# å“åº”æ ¼å¼åŒ–èŠ‚ç‚¹
# ============================================

def response_formatter(state: AgentState) -> dict:
    """æ ¼å¼åŒ–æœ€ç»ˆå“åº”"""
    if state["intent"] == "terminal_command":
        if state.get("error"):
            response = f"âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥\n\nå‘½ä»¤: {state['command']}\né”™è¯¯: {state['error']}"
        else:
            response = f"âœ… å‘½ä»¤æ‰§è¡ŒæˆåŠŸ\n\nå‘½ä»¤: {state['command']}\n\nè¾“å‡º:\n{state['command_output']}"
    
    elif state["intent"] == "multi_step_command":
        response = "âœ… å¤šæ­¥éª¤ä»»åŠ¡æ‰§è¡Œç»“æœ:\n\n"
        
        if state.get("needs_file_creation"):
            response += f"ğŸ“„ åˆ›å»ºæ–‡ä»¶: {state.get('file_path', '')}\n\n"
        
        outputs = state.get("command_outputs", [])
        for idx, output in enumerate(outputs, 1):
            status = "âœ…" if output["success"] else "âŒ"
            response += f"{status} å‘½ä»¤ {idx}: {output['command']}\n"
            if output["success"]:
                response += f"è¾“å‡º:\n{output['output']}\n\n"
            else:
                response += f"é”™è¯¯: {output['error']}\n\n"
    
    elif state["intent"] == "mcp_tool_call":
        result = json.loads(state.get("mcp_result", "{}"))
        
        if result.get("success"):
            response = format_mcp_success_response(state['mcp_tool'], result)
        else:
            response = f"âŒ MCPå·¥å…·æ‰§è¡Œå¤±è´¥\n\n"
            response += f"å·¥å…·: {state['mcp_tool']}\n"
            response += f"é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
    
    else:
        response = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å¤„ç†è¿™ä¸ªè¯·æ±‚ã€‚"

    print(f"[æ ¼å¼åŒ–å“åº”] å®Œæˆ")
    return {"response": response}


def format_mcp_success_response(tool_name: str, result: dict) -> str:
    """æ ¼å¼åŒ–MCPæˆåŠŸå“åº”"""
    response = f"âœ… MCPå·¥å…·æ‰§è¡ŒæˆåŠŸ\n\nå·¥å…·: {tool_name}\n\n"
    
    if tool_name == "fs_read":
        content = result.get('content', '')
        lines = result.get('lines', 0)
        size = result.get('size', 0)
        response += f"æ–‡ä»¶å¤§å°: {size} å­—èŠ‚\n"
        response += f"è¡Œæ•°: {lines}\n\n"
        response += f"å†…å®¹:\n{'-' * 60}\n{content}\n{'-' * 60}"
    
    elif tool_name == "fs_list":
        response += f"ç›®å½•: {result.get('path', '.')}\n"
        response += f"æ‰¾åˆ° {result['total_files']} ä¸ªæ–‡ä»¶\n\n"
        for f in result['files'][:20]:
            response += f"  ğŸ“„ {f['name']:<40} {f['size_human']:>10}  {f['modified']}\n"
        if result['total_files'] > 20:
            response += f"\n... è¿˜æœ‰ {result['total_files'] - 20} ä¸ªæ–‡ä»¶"
    
    elif tool_name == "fs_search":
        response += f"æ‰¾åˆ° {result['total']} ä¸ªåŒ¹é…æ–‡ä»¶\n\n"
        for f in result['matches'][:15]:
            response += f"  ğŸ“ {f['name']} ({f['size_human']})\n"
            if f.get('content_matched'):
                response += f"     åŒ¹é…è¡Œ:\n"
                for line_num, line_content in f.get('matched_lines', [])[:3]:
                    response += f"       {line_num}: {line_content.strip()[:60]}...\n"
        if result['total'] > 15:
            response += f"\n... è¿˜æœ‰ {result['total'] - 15} ä¸ªæ–‡ä»¶"
    
    elif tool_name == "fs_write":
        response += f"æ–‡ä»¶è·¯å¾„: {result.get('path', '')}\n"
        response += f"å†™å…¥å¤§å°: {result.get('size', 0)} å­—èŠ‚\n"
        response += f"è¡Œæ•°: {result.get('lines', 0)}\n"
        response += f"æ¨¡å¼: {result.get('mode', 'write')}"
    
    elif tool_name == "fs_info":
        response += f"æ–‡ä»¶å: {result.get('name', '')}\n"
        response += f"è·¯å¾„: {result.get('path', '')}\n"
        response += f"å¤§å°: {result.get('size_human', '')}\n"
        response += f"ä¿®æ”¹æ—¶é—´: {result.get('modified', '')}\n"
        response += f"åˆ›å»ºæ—¶é—´: {result.get('created', '')}\n"
        response += f"ç±»å‹: {'æ–‡ä»¶' if result.get('is_file') else 'ç›®å½•'}"
    
    elif tool_name.startswith("desktop_"):
        response += f"ç»“æœ:\n{json.dumps(result.get('result', {}), ensure_ascii=False, indent=2)}"
    
    else:
        response += f"ç»“æœ:\n{json.dumps(result, ensure_ascii=False, indent=2)}"
    
    return response
