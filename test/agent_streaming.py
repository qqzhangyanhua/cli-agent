"""
æµå¼è¾“å‡ºæ”¯æŒæ¨¡å—
æä¾›æµå¼ LLM è°ƒç”¨å’Œå®æ—¶è¾“å‡ºåŠŸèƒ½
"""

import sys
from typing import Iterator, Optional
from langchain_core.messages import BaseMessage, HumanMessage
from agent_llm import llm


def stream_llm_response(messages: list, print_output: bool = True) -> tuple[str, str]:
    """
    æµå¼è°ƒç”¨ LLM å¹¶å®æ—¶è¾“å‡º

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        print_output: æ˜¯å¦å®æ—¶æ‰“å°è¾“å‡º

    Returns:
        (å®Œæ•´å“åº”æ–‡æœ¬, é”™è¯¯ä¿¡æ¯)
    """
    try:
        full_response = ""

        if print_output:
            print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)

        # ä½¿ç”¨ stream æ–¹æ³•è¿›è¡Œæµå¼è°ƒç”¨
        for chunk in llm.stream(messages):
            # æå–å†…å®¹
            if hasattr(chunk, 'content'):
                content = chunk.content
            elif isinstance(chunk, str):
                content = chunk
            else:
                content = str(chunk)

            # ç´¯ç§¯å®Œæ•´å“åº”
            full_response += content

            # å®æ—¶è¾“å‡º
            if print_output:
                print(content, end="", flush=True)

        if print_output:
            print()  # æ¢è¡Œ

        return full_response, ""

    except Exception as e:
        error_msg = f"æµå¼è°ƒç”¨å¤±è´¥: {str(e)}"
        if print_output:
            print(f"\nâŒ {error_msg}")
        return "", error_msg


def stream_llm_with_prompt(prompt: str, print_output: bool = True) -> tuple[str, str]:
    """
    ä¾¿æ·å‡½æ•°ï¼šä½¿ç”¨æç¤ºè¯è¿›è¡Œæµå¼è°ƒç”¨

    Args:
        prompt: æç¤ºè¯
        print_output: æ˜¯å¦å®æ—¶æ‰“å°

    Returns:
        (å®Œæ•´å“åº”, é”™è¯¯ä¿¡æ¯)
    """
    messages = [HumanMessage(content=prompt)]
    return stream_llm_response(messages, print_output)


def print_streaming(text: str, prefix: str = "", color_code: str = ""):
    """
    æµå¼æ‰“å°æ–‡æœ¬ï¼ˆæ¨¡æ‹Ÿæ‰“å­—æœºæ•ˆæœï¼‰

    Args:
        text: è¦æ‰“å°çš„æ–‡æœ¬
        prefix: å‰ç¼€ï¼ˆå¦‚ "ğŸ¤– åŠ©æ‰‹: "ï¼‰
        color_code: ANSI é¢œè‰²ä»£ç ï¼ˆå¯é€‰ï¼‰
    """
    import time

    if prefix:
        print(prefix, end="", flush=True)

    for char in text:
        if color_code:
            print(f"{color_code}{char}\033[0m", end="", flush=True)
        else:
            print(char, end="", flush=True)

        # æ ¹æ®å­—ç¬¦ç±»å‹è°ƒæ•´å»¶è¿Ÿ
        if char in "ã€‚ï¼ï¼Ÿ\n":
            time.sleep(0.05)  # æ ‡ç‚¹ç¬¦å·åç¨å¾®åœé¡¿
        elif char in "ï¼Œã€ï¼›ï¼š":
            time.sleep(0.03)
        else:
            time.sleep(0.01)  # æ™®é€šå­—ç¬¦å¿«é€Ÿè¾“å‡º

    print()  # ç»“æŸæ¢è¡Œ


def format_streaming_output(content: str, role: str = "assistant") -> None:
    """
    æ ¼å¼åŒ–æµå¼è¾“å‡º

    Args:
        content: è¾“å‡ºå†…å®¹
        role: è§’è‰²ï¼ˆassistant/user/systemï¼‰
    """
    if role == "assistant":
        print("â”€" * 80)
        print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
    elif role == "user":
        print("ğŸ‘¤ ä½ : ", end="", flush=True)

    print(content)

    if role == "assistant":
        print("â”€" * 80)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("æµ‹è¯•æµå¼è¾“å‡ºåŠŸèƒ½\n")

    # æµ‹è¯•1ï¼šæµå¼é—®ç­”
    print("=" * 60)
    print("æµ‹è¯•1: æµå¼é—®ç­”")
    print("=" * 60)

    response, error = stream_llm_with_prompt(
        "ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯ Python",
        print_output=True
    )

    if not error:
        print(f"\nâœ… å®Œæ•´å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")

    # æµ‹è¯•2ï¼šä¸æ‰“å°æ¨¡å¼
    print("\n" + "=" * 60)
    print("æµ‹è¯•2: ä¸æ‰“å°æ¨¡å¼ï¼ˆåå°å¤„ç†ï¼‰")
    print("=" * 60)

    response2, error2 = stream_llm_with_prompt(
        "ä»€ä¹ˆæ˜¯ Git",
        print_output=False
    )

    if not error2:
        print(f"âœ… åå°è·å–å“åº”: {response2[:100]}...")
        print(f"   å®Œæ•´é•¿åº¦: {len(response2)} å­—ç¬¦")
