"""
AIæ™ºèƒ½ä½“ç»ˆç«¯æ§åˆ¶å·¥å…· - äº¤äº’å¼ç‰ˆæœ¬ + MCPé›†æˆ
æ”¯æŒå¯¹è¯åŠŸèƒ½ã€è®°å¿†åŠŸèƒ½å’ŒMCPå·¥å…·é›†æˆ

åŠŸèƒ½:
- æ–‡ä»¶ç³»ç»Ÿè®¿é—®ï¼ˆè¯»/å†™/åˆ—è¡¨/æœç´¢ï¼‰
- æ¡Œé¢æ§åˆ¶ï¼ˆdesktop-commanderï¼‰
- ç»ˆç«¯å‘½ä»¤æ‰§è¡Œ
- æ™ºèƒ½å¯¹è¯å’Œè®°å¿†

è¿è¡Œ: python3 terminal_agent_interactive.py
"""

import subprocess
import json
from typing import TypedDict, Literal, List
from datetime import datetime
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from mcp_manager import mcp_manager  # MCPç®¡ç†å™¨

# ============================================
# é…ç½®åŒº
# ============================================
# é€šç”¨LLMé…ç½® - ç”¨äºæ„å›¾åˆ†æã€é—®ç­”ç­‰
LLM_CONFIG = {
    "model": "kimi-k2-0905-preview",
    "base_url": "https://api.moonshot.cn/v1",
    "api_key": "sk-",
    "temperature": 0,
}

# ä»£ç ç”Ÿæˆä¸“ç”¨LLMé…ç½® - ç”¨äºç”Ÿæˆå‘½ä»¤å’Œä»£ç 
LLM_CONFIG2 = {
    "model": "claude-3-5-sonnet",  # ä½¿ç”¨Claudeä½œä¸ºä»£ç ç”Ÿæˆæ¨¡å‹
    "base_url": "https://sdwfger.edu.kg/v1",
    "api_key": "sk-",
    "temperature": 0,
}


# ============================================
# æ•°æ®ç»“æ„å®šä¹‰
# ============================================
class AgentState(TypedDict):
    """æ™ºèƒ½ä½“çŠ¶æ€"""
    user_input: str
    intent: Literal["terminal_command", "multi_step_command", "mcp_tool_call", "question", "unknown"]
    command: str
    commands: list
    command_output: str
    command_outputs: list
    response: str
    error: str
    needs_file_creation: bool
    file_path: str
    file_content: str
    chat_history: list  # å¯¹è¯å†å²è®°å¿†
    # MCPç›¸å…³å­—æ®µ
    mcp_tool: str  # MCPå·¥å…·åç§°
    mcp_params: dict  # MCPå·¥å…·å‚æ•°
    mcp_result: str  # MCPæ‰§è¡Œç»“æœ


class ConversationMemory:
    """å¯¹è¯è®°å¿†ç®¡ç†"""
    def __init__(self, max_history=10):
        self.history: List[dict] = []
        self.max_history = max_history
        self.command_history: List[dict] = []  # å‘½ä»¤æ‰§è¡Œå†å²
    
    def add_interaction(self, user_input: str, agent_response: str, intent: str):
        """æ·»åŠ ä¸€æ¬¡äº¤äº’åˆ°å†å²"""
        self.history.append({
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "user": user_input,
            "agent": agent_response,
            "intent": intent
        })
        
        # ä¿æŒå†å²è®°å½•åœ¨é™åˆ¶èŒƒå›´å†…
        if len(self.history) > self.max_history:
            self.history.pop(0)
    
    def add_command(self, command: str, output: str, success: bool):
        """è®°å½•å‘½ä»¤æ‰§è¡Œå†å²"""
        self.command_history.append({
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "command": command,
            "output": output,
            "success": success
        })
        
        if len(self.command_history) > 20:
            self.command_history.pop(0)
    
    def get_context_string(self) -> str:
        """è·å–å¯¹è¯ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
        if not self.history:
            return "è¿™æ˜¯æˆ‘ä»¬çš„ç¬¬ä¸€æ¬¡å¯¹è¯ã€‚"
        
        context = "å¯¹è¯å†å²:\n"
        for idx, interaction in enumerate(self.history[-5:], 1):  # åªå–æœ€è¿‘5æ¡
            context += f"{idx}. ç”¨æˆ·: {interaction['user']}\n"
            context += f"   åŠ©æ‰‹: {interaction['agent'][:100]}...\n"  # æˆªæ–­é•¿å“åº”
        
        return context
    
    def get_recent_commands(self, n=3) -> str:
        """è·å–æœ€è¿‘çš„å‘½ä»¤å†å²"""
        if not self.command_history:
            return "æš‚æ— å‘½ä»¤æ‰§è¡Œå†å²ã€‚"
        
        recent = self.command_history[-n:]
        result = "æœ€è¿‘æ‰§è¡Œçš„å‘½ä»¤:\n"
        for cmd in recent:
            status = "âœ…" if cmd["success"] else "âŒ"
            result += f"{status} {cmd['command']}\n"
        
        return result
    
    def clear(self):
        """æ¸…ç©ºè®°å¿†"""
        self.history.clear()
        self.command_history.clear()


# ============================================
# å…¨å±€è®°å¿†å®ä¾‹
# ============================================
memory = ConversationMemory(max_history=10)


# ============================================
# åˆå§‹åŒ– LLM
# ============================================
# é€šç”¨LLM - ç”¨äºæ„å›¾åˆ†æã€é—®ç­”ç­‰
llm = ChatOpenAI(
    model=LLM_CONFIG["model"],
    base_url=LLM_CONFIG["base_url"],
    api_key=LLM_CONFIG["api_key"],
    temperature=LLM_CONFIG["temperature"],
    default_headers={
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
)

# ä»£ç ç”Ÿæˆä¸“ç”¨LLM - ç”¨äºç”Ÿæˆå‘½ä»¤å’Œä»£ç 
llm_code = ChatOpenAI(
    model=LLM_CONFIG2["model"],
    base_url=LLM_CONFIG2["base_url"],
    api_key=LLM_CONFIG2["api_key"],
    temperature=LLM_CONFIG2["temperature"],
    default_headers={
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
)


# ============================================
# å·¥å…·å‡½æ•° - ç»ˆç«¯å‘½ä»¤æ‰§è¡Œ
# ============================================

def execute_terminal_command(command: str) -> dict:
    """å®‰å…¨åœ°æ‰§è¡Œç»ˆç«¯å‘½ä»¤"""
    dangerous_commands = ["rm -rf", "sudo rm", "chmod 777", "format", "del /f"]
    for dangerous in dangerous_commands:
        if dangerous in command.lower():
            return {
                "success": False,
                "output": "",
                "error": f"âš ï¸ æ‹’ç»æ‰§è¡Œå±é™©å‘½ä»¤: {command}"
            }

    try:
        result = subprocess.run(
            command,
            shell=True,
            capture_output=True,
            text=True,
            timeout=10,
            cwd="/Users/zhangyanhua/Desktop/AI/tushare/quantification/example"
        )
        
        output = result.stdout if result.stdout else "(å‘½ä»¤æ‰§è¡ŒæˆåŠŸï¼Œæ— è¾“å‡º)"
        
        # è®°å½•åˆ°å‘½ä»¤å†å²
        memory.add_command(command, output, result.returncode == 0)
        
        return {
            "success": result.returncode == 0,
            "output": output,
            "error": result.stderr
        }
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "output": "",
            "error": "â±ï¸ å‘½ä»¤æ‰§è¡Œè¶…æ—¶(>10ç§’)"
        }
    except Exception as e:
        return {
            "success": False,
            "output": "",
            "error": f"âŒ æ‰§è¡Œå¤±è´¥: {str(e)}"
        }


# ============================================
# èŠ‚ç‚¹å‡½æ•°å®šä¹‰
# ============================================

def intent_analyzer(state: AgentState) -> dict:
    """åˆ†æç”¨æˆ·æ„å›¾ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰"""
    user_input = state["user_input"]
    context = memory.get_context_string()

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ç»ˆç«¯åŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·è¾“å…¥å’Œå¯¹è¯å†å²ï¼Œåˆ†æç”¨æˆ·æ„å›¾ã€‚

{context}

å½“å‰ç”¨æˆ·è¾“å…¥: {user_input}

åˆ¤æ–­è§„åˆ™:
- å¦‚æœç”¨æˆ·æƒ³è¯»å–æ–‡ä»¶ã€å†™å…¥æ–‡ä»¶ã€åˆ—å‡ºç›®å½•ã€æœç´¢æ–‡ä»¶ã€è·å–æ–‡ä»¶ä¿¡æ¯ -> mcp_tool_call
- å¦‚æœç”¨æˆ·æƒ³æˆªå›¾ã€æ“ä½œå‰ªè´´æ¿ã€æ‰§è¡Œæ¡Œé¢å‘½ä»¤ -> mcp_tool_call
- å¦‚æœç”¨æˆ·æƒ³æ‰§è¡Œç³»ç»Ÿå‘½ä»¤ã€è¿è¡Œç¨‹åº -> terminal_command
- å¦‚æœç”¨æˆ·éœ€è¦åˆ›å»ºä»£ç æ–‡ä»¶å¹¶æ‰§è¡Œã€æˆ–è€…éœ€è¦å¤šä¸ªæ­¥éª¤å®Œæˆä»»åŠ¡ -> multi_step_command
- å¦‚æœç”¨æˆ·åœ¨é—®é—®é¢˜ã€å¯»æ±‚è§£é‡Šã€éœ€è¦å»ºè®®ã€æˆ–è€…å¼•ç”¨ä¹‹å‰çš„å¯¹è¯ -> question

åªè¿”å›ä¸€ä¸ªè¯: 'mcp_tool_call', 'terminal_command', 'multi_step_command' æˆ– 'question'

æ„å›¾:"""

    result = llm.invoke([HumanMessage(content=prompt)])
    intent = result.content.strip().lower()

    if intent not in ["mcp_tool_call", "terminal_command", "multi_step_command", "question"]:
        intent = "question"  # é»˜è®¤ä¸ºé—®ç­”

    print(f"\n[æ„å›¾åˆ†æ] {user_input[:50]}...")
    print(f"           ä½¿ç”¨æ¨¡å‹: {LLM_CONFIG['model']}")
    print(f"           æ„å›¾: {intent}")

    return {"intent": intent}


def command_generator(state: AgentState) -> dict:
    """ç”Ÿæˆç»ˆç«¯å‘½ä»¤ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰"""
    user_input = state["user_input"]
    recent_commands = memory.get_recent_commands()

    prompt = f"""å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€è¯·æ±‚è½¬æ¢ä¸ºç»ˆç«¯å‘½ä»¤ã€‚

{recent_commands}

å½“å‰è¯·æ±‚: {user_input}

ç¤ºä¾‹:
- "åˆ—å‡ºå½“å‰ç›®å½•çš„æ‰€æœ‰æ–‡ä»¶" -> ls -la
- "æŸ¥çœ‹Pythonç‰ˆæœ¬" -> python3 --version
- "æ˜¾ç¤ºå½“å‰è·¯å¾„" -> pwd

åªè¿”å›å‘½ä»¤æœ¬èº«ï¼Œä¸è¦è§£é‡Š:"""

    result = llm_code.invoke([HumanMessage(content=prompt)])  # ä½¿ç”¨ä»£ç ç”ŸæˆLLM
    command = result.content.strip()

    print(f"[å‘½ä»¤ç”Ÿæˆ] {command}")
    print(f"           ä½¿ç”¨æ¨¡å‹: {LLM_CONFIG2['model']}")

    return {"command": command}


def multi_step_planner(state: AgentState) -> dict:
    """å¤šæ­¥éª¤è§„åˆ’ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰"""
    user_input = state["user_input"]
    recent_commands = memory.get_recent_commands()

    prompt = f"""åˆ†æç”¨æˆ·è¯·æ±‚ï¼Œè¿”å›JSONæ ¼å¼çš„æ‰§è¡Œè®¡åˆ’ã€‚

{recent_commands}

ç”¨æˆ·è¯·æ±‚: {user_input}

è¿”å›JSONå¯¹è±¡:
{{
  "needs_file_creation": true/false,
  "file_path": "æ–‡ä»¶è·¯å¾„",
  "file_content": "æ–‡ä»¶å†…å®¹",
  "commands": ["å‘½ä»¤1", "å‘½ä»¤2"]
}}

ç¤ºä¾‹:
è¾“å…¥: "åˆ›å»ºä¸€ä¸ªPythonæ–‡ä»¶test.pyï¼Œæ‰“å°1åˆ°10ï¼Œç„¶åæ‰§è¡Œ"
è¾“å‡º:
{{
  "needs_file_creation": true,
  "file_path": "test.py",
  "file_content": "for i in range(1, 11):\\n    print(i)",
  "commands": ["python3 test.py"]
}}

åªè¿”å›JSON:"""

    result = llm_code.invoke([HumanMessage(content=prompt)])  # ä½¿ç”¨ä»£ç ç”ŸæˆLLM
    plan_text = result.content.strip()
    
    if "```json" in plan_text:
        plan_text = plan_text.split("```json")[1].split("```")[0].strip()
    elif "```" in plan_text:
        plan_text = plan_text.split("```")[1].split("```")[0].strip()
    
    try:
        plan = json.loads(plan_text)
        print(f"[å¤šæ­¥éª¤è§„åˆ’] ä½¿ç”¨æ¨¡å‹: {LLM_CONFIG2['model']}")
        print(f"            éœ€è¦åˆ›å»ºæ–‡ä»¶: {plan.get('needs_file_creation', False)}")
        print(f"            å‘½ä»¤æ•°é‡: {len(plan.get('commands', []))}")
        
        return {
            "needs_file_creation": plan.get("needs_file_creation", False),
            "file_path": plan.get("file_path", ""),
            "file_content": plan.get("file_content", ""),
            "commands": plan.get("commands", [])
        }
    except json.JSONDecodeError:
        print(f"[å¤šæ­¥éª¤è§„åˆ’] JSONè§£æå¤±è´¥")
        return {
            "needs_file_creation": False,
            "file_path": "",
            "file_content": "",
            "commands": [],
            "error": "æ— æ³•è§£ææ‰§è¡Œè®¡åˆ’"
        }


def file_creator(state: AgentState) -> dict:
    """åˆ›å»ºæ–‡ä»¶"""
    file_path = state["file_path"]
    file_content = state["file_content"]
    
    print(f"[æ–‡ä»¶åˆ›å»º] åˆ›å»ºæ–‡ä»¶: {file_path}")
    
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(file_content)
        print(f"[æ–‡ä»¶åˆ›å»º] âœ… æˆåŠŸåˆ›å»ºæ–‡ä»¶: {file_path}")
        return {"error": ""}
    except Exception as e:
        error_msg = f"æ–‡ä»¶åˆ›å»ºå¤±è´¥: {str(e)}"
        print(f"[æ–‡ä»¶åˆ›å»º] âŒ {error_msg}")
        return {"error": error_msg}


def mcp_tool_planner(state: AgentState) -> dict:
    """è§„åˆ’MCPå·¥å…·è°ƒç”¨"""
    user_input = state["user_input"]
    
    # è·å–å¯ç”¨å·¥å…·åˆ—è¡¨
    available_tools = mcp_manager.list_available_tools()
    tools_desc = "\n".join([
        f"- {t['name']}: {t['description']} (å‚æ•°: {', '.join(t['params'])})"
        for t in available_tools
    ])
    
    prompt = f"""åˆ†æç”¨æˆ·è¯·æ±‚ï¼Œé€‰æ‹©åˆé€‚çš„MCPå·¥å…·å¹¶è¿”å›JSONæ ¼å¼ã€‚

å¯ç”¨å·¥å…·:
{tools_desc}

ç”¨æˆ·è¯·æ±‚: {user_input}

è¿”å›JSONæ ¼å¼:
{{
  "tool": "å·¥å…·åç§°",
  "params": {{å‚æ•°å: å‚æ•°å€¼}}
}}

ç¤ºä¾‹:
è¾“å…¥: "è¯»å–README.mdæ–‡ä»¶"
è¾“å‡º: {{
  "tool": "fs_read",
  "params": {{"file_path": "README.md"}}
}}

è¾“å…¥: "åˆ—å‡ºå½“å‰ç›®å½•çš„æ‰€æœ‰Pythonæ–‡ä»¶"
è¾“å‡º: {{
  "tool": "fs_list",
  "params": {{"dir_path": ".", "pattern": "*.py"}}
}}

è¾“å…¥: "æœç´¢åŒ…å«LLM_CONFIGçš„Pythonæ–‡ä»¶"
è¾“å‡º: {{
  "tool": "fs_search",
  "params": {{"dir_path": ".", "filename_pattern": "*.py", "content_search": "LLM_CONFIG"}}
}}

åªè¿”å›JSON:"""
    
    result = llm_code.invoke([HumanMessage(content=prompt)])
    plan_text = result.content.strip()
    
    # æå–JSON
    if "```json" in plan_text:
        plan_text = plan_text.split("```json")[1].split("```")[0].strip()
    elif "```" in plan_text:
        plan_text = plan_text.split("```")[1].split("```")[0].strip()
    
    try:
        plan = json.loads(plan_text)
        print(f"[MCPå·¥å…·è§„åˆ’] ä½¿ç”¨æ¨¡å‹: {LLM_CONFIG2['model']}")
        print(f"            å·¥å…·: {plan.get('tool', 'unknown')}")
        print(f"            å‚æ•°: {plan.get('params', {})}")
        
        return {
            "mcp_tool": plan.get("tool", ""),
            "mcp_params": plan.get("params", {})
        }
    except json.JSONDecodeError as e:
        print(f"[MCPå·¥å…·è§„åˆ’] JSONè§£æå¤±è´¥: {e}")
        return {
            "mcp_tool": "",
            "mcp_params": {},
            "error": "æ— æ³•è§£æMCPå·¥å…·è§„åˆ’"
        }


def mcp_tool_executor(state: AgentState) -> dict:
    """æ‰§è¡ŒMCPå·¥å…·"""
    tool_name = state["mcp_tool"]
    params = state["mcp_params"]
    
    print(f"[MCPå·¥å…·æ‰§è¡Œ] å·¥å…·: {tool_name}")
    print(f"            å‚æ•°: {params}")
    
    try:
        result = mcp_manager.call_tool(tool_name, **params)
        
        if result.get("success"):
            print(f"[MCPå·¥å…·æ‰§è¡Œ] âœ… æˆåŠŸ")
        else:
            print(f"[MCPå·¥å…·æ‰§è¡Œ] âŒ å¤±è´¥: {result.get('error')}")
        
        return {"mcp_result": json.dumps(result, ensure_ascii=False)}
    
    except Exception as e:
        error_result = {"success": False, "error": str(e)}
        print(f"[MCPå·¥å…·æ‰§è¡Œ] âŒ å¼‚å¸¸: {e}")
        return {"mcp_result": json.dumps(error_result, ensure_ascii=False)}


def multi_command_executor(state: AgentState) -> dict:
    """æ‰§è¡Œå¤šä¸ªç»ˆç«¯å‘½ä»¤"""
    commands = state["commands"]
    outputs = []
    
    print(f"[å¤šå‘½ä»¤æ‰§è¡Œ] å…±{len(commands)}ä¸ªå‘½ä»¤")
    
    for idx, command in enumerate(commands, 1):
        print(f"[å¤šå‘½ä»¤æ‰§è¡Œ] æ‰§è¡Œç¬¬{idx}ä¸ªå‘½ä»¤: {command}")
        result = execute_terminal_command(command)
        
        outputs.append({
            "command": command,
            "success": result["success"],
            "output": result["output"],
            "error": result["error"]
        })
        
        if result["success"]:
            print(f"[å¤šå‘½ä»¤æ‰§è¡Œ] âœ… ç¬¬{idx}ä¸ªå‘½ä»¤æ‰§è¡ŒæˆåŠŸ")
        else:
            print(f"[å¤šå‘½ä»¤æ‰§è¡Œ] âŒ ç¬¬{idx}ä¸ªå‘½ä»¤æ‰§è¡Œå¤±è´¥: {result['error']}")
    
    return {"command_outputs": outputs}


def command_executor(state: AgentState) -> dict:
    """æ‰§è¡Œå•ä¸ªç»ˆç«¯å‘½ä»¤"""
    command = state["command"]

    print(f"[æ‰§è¡Œå‘½ä»¤] {command}")

    result = execute_terminal_command(command)

    if result["success"]:
        print(f"[æ‰§è¡ŒæˆåŠŸ] è¾“å‡ºé•¿åº¦: {len(result['output'])} å­—ç¬¦")
        return {
            "command_output": result["output"],
            "error": ""
        }
    else:
        print(f"[æ‰§è¡Œå¤±è´¥] {result['error']}")
        return {
            "command_output": "",
            "error": result["error"]
        }


def response_formatter(state: AgentState) -> dict:
    """æ ¼å¼åŒ–æœ€ç»ˆå“åº”"""
    if state["intent"] == "terminal_command":
        if state.get("error"):
            response = f"âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥\n\nå‘½ä»¤: {state['command']}\né”™è¯¯: {state['error']}"
        else:
            response = f"âœ… å‘½ä»¤æ‰§è¡ŒæˆåŠŸ\n\nå‘½ä»¤: {state['command']}\n\nè¾“å‡º:\n{state['command_output']}"
    
    elif state["intent"] == "multi_step_command":
        response = "âœ… å¤šæ­¥éª¤ä»»åŠ¡æ‰§è¡Œç»“æœ:\n\n"
        
        if state.get("needs_file_creation"):
            response += f"ğŸ“„ åˆ›å»ºæ–‡ä»¶: {state.get('file_path', '')}\n\n"
        
        outputs = state.get("command_outputs", [])
        for idx, output in enumerate(outputs, 1):
            status = "âœ…" if output["success"] else "âŒ"
            response += f"{status} å‘½ä»¤ {idx}: {output['command']}\n"
            if output["success"]:
                response += f"è¾“å‡º:\n{output['output']}\n\n"
            else:
                response += f"é”™è¯¯: {output['error']}\n\n"
    
    elif state["intent"] == "mcp_tool_call":
        result = json.loads(state.get("mcp_result", "{}"))
        
        if result.get("success"):
            response = f"âœ… MCPå·¥å…·æ‰§è¡ŒæˆåŠŸ\n\n"
            response += f"å·¥å…·: {state['mcp_tool']}\n\n"
            
            # æ ¹æ®ä¸åŒå·¥å…·ç±»å‹æ ¼å¼åŒ–è¾“å‡º
            if state['mcp_tool'] == "fs_read":
                content = result.get('content', '')
                lines = result.get('lines', 0)
                size = result.get('size', 0)
                response += f"æ–‡ä»¶å¤§å°: {size} å­—èŠ‚\n"
                response += f"è¡Œæ•°: {lines}\n\n"
                response += f"å†…å®¹:\n{'-' * 60}\n{content}\n{'-' * 60}"
            
            elif state['mcp_tool'] == "fs_list":
                response += f"ç›®å½•: {result.get('path', '.')}\n"
                response += f"æ‰¾åˆ° {result['total_files']} ä¸ªæ–‡ä»¶\n\n"
                for f in result['files'][:20]:
                    response += f"  ğŸ“„ {f['name']:<40} {f['size_human']:>10}  {f['modified']}\n"
                if result['total_files'] > 20:
                    response += f"\n... è¿˜æœ‰ {result['total_files'] - 20} ä¸ªæ–‡ä»¶"
            
            elif state['mcp_tool'] == "fs_search":
                response += f"æ‰¾åˆ° {result['total']} ä¸ªåŒ¹é…æ–‡ä»¶\n\n"
                for f in result['matches'][:15]:
                    response += f"  ğŸ“ {f['name']} ({f['size_human']})\n"
                    if f.get('content_matched'):
                        response += f"     åŒ¹é…è¡Œ:\n"
                        for line_num, line_content in f.get('matched_lines', [])[:3]:
                            response += f"       {line_num}: {line_content.strip()[:60]}...\n"
                if result['total'] > 15:
                    response += f"\n... è¿˜æœ‰ {result['total'] - 15} ä¸ªæ–‡ä»¶"
            
            elif state['mcp_tool'] == "fs_write":
                response += f"æ–‡ä»¶è·¯å¾„: {result.get('path', '')}\n"
                response += f"å†™å…¥å¤§å°: {result.get('size', 0)} å­—èŠ‚\n"
                response += f"è¡Œæ•°: {result.get('lines', 0)}\n"
                response += f"æ¨¡å¼: {result.get('mode', 'write')}"
            
            elif state['mcp_tool'] == "fs_info":
                response += f"æ–‡ä»¶å: {result.get('name', '')}\n"
                response += f"è·¯å¾„: {result.get('path', '')}\n"
                response += f"å¤§å°: {result.get('size_human', '')}\n"
                response += f"ä¿®æ”¹æ—¶é—´: {result.get('modified', '')}\n"
                response += f"åˆ›å»ºæ—¶é—´: {result.get('created', '')}\n"
                response += f"ç±»å‹: {'æ–‡ä»¶' if result.get('is_file') else 'ç›®å½•'}"
            
            elif state['mcp_tool'].startswith("desktop_"):
                # æ¡Œé¢æ§åˆ¶å·¥å…·ç»“æœ
                response += f"ç»“æœ:\n{json.dumps(result.get('result', {}), ensure_ascii=False, indent=2)}"
            
            else:
                response += f"ç»“æœ:\n{json.dumps(result, ensure_ascii=False, indent=2)}"
        else:
            response = f"âŒ MCPå·¥å…·æ‰§è¡Œå¤±è´¥\n\n"
            response += f"å·¥å…·: {state['mcp_tool']}\n"
            response += f"é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
    
    else:
        response = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å¤„ç†è¿™ä¸ªè¯·æ±‚ã€‚"

    print(f"[æ ¼å¼åŒ–å“åº”] å®Œæˆ")

    return {"response": response}


def question_answerer(state: AgentState) -> dict:
    """å›ç­”ç”¨æˆ·é—®é¢˜ï¼ˆå¸¦ä¸Šä¸‹æ–‡å’Œè®°å¿†ï¼‰"""
    user_input = state["user_input"]
    context = memory.get_context_string()
    recent_commands = memory.get_recent_commands()

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIç»ˆç«¯åŠ©æ‰‹ã€‚å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¯¹è¯å†å²æä¾›æ›´å¥½çš„å¸®åŠ©ã€‚

{context}

{recent_commands}

å½“å‰é—®é¢˜: {user_input}

è¯·ç®€æ´ä½†å…¨é¢åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœç”¨æˆ·æåˆ°"åˆšæ‰"ã€"ä¹‹å‰"ç­‰è¯ï¼Œè¯·å‚è€ƒå¯¹è¯å†å²ã€‚

å›ç­”:"""

    result = llm.invoke([HumanMessage(content=prompt)])
    response = result.content

    print(f"[é—®é¢˜å›ç­”] ç”Ÿæˆå›ç­”")
    print(f"           ä½¿ç”¨æ¨¡å‹: {LLM_CONFIG['model']}")

    return {"response": response}


# ============================================
# è·¯ç”±å‡½æ•°
# ============================================

def route_by_intent(state: AgentState) -> str:
    """æ ¹æ®æ„å›¾è·¯ç”±"""
    intent = state["intent"]
    if intent == "terminal_command":
        return "generate_command"
    elif intent == "multi_step_command":
        return "plan_steps"
    elif intent == "mcp_tool_call":
        return "plan_mcp_tool"
    elif intent == "question":
        return "answer_question"
    else:
        return "format_response"


def route_after_planning(state: AgentState) -> str:
    """è§„åˆ’åçš„è·¯ç”±"""
    if state.get("needs_file_creation", False):
        return "create_file"
    else:
        return "execute_multi_commands"


# ============================================
# æ„å»ºå·¥ä½œæµ
# ============================================

def build_agent() -> StateGraph:
    """æ„å»ºAIæ™ºèƒ½ä½“å·¥ä½œæµ"""

    workflow = StateGraph(AgentState)

    # æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
    workflow.add_node("analyze_intent", intent_analyzer)
    workflow.add_node("generate_command", command_generator)
    workflow.add_node("execute_command", command_executor)
    workflow.add_node("plan_steps", multi_step_planner)
    workflow.add_node("create_file", file_creator)
    workflow.add_node("execute_multi_commands", multi_command_executor)
    workflow.add_node("plan_mcp_tool", mcp_tool_planner)  # MCPå·¥å…·è§„åˆ’
    workflow.add_node("execute_mcp_tool", mcp_tool_executor)  # MCPå·¥å…·æ‰§è¡Œ
    workflow.add_node("format_response", response_formatter)
    workflow.add_node("answer_question", question_answerer)

    workflow.set_entry_point("analyze_intent")

    # æ„å›¾è·¯ç”±
    workflow.add_conditional_edges(
        "analyze_intent",
        route_by_intent,
        {
            "generate_command": "generate_command",
            "plan_steps": "plan_steps",
            "plan_mcp_tool": "plan_mcp_tool",  # MCPå·¥å…·è·¯å¾„
            "answer_question": "answer_question",
            "format_response": "format_response"
        }
    )

    # ç»ˆç«¯å‘½ä»¤è·¯å¾„
    workflow.add_edge("generate_command", "execute_command")
    workflow.add_edge("execute_command", "format_response")
    
    # å¤šæ­¥éª¤å‘½ä»¤è·¯å¾„
    workflow.add_conditional_edges(
        "plan_steps",
        route_after_planning,
        {
            "create_file": "create_file",
            "execute_multi_commands": "execute_multi_commands"
        }
    )
    workflow.add_edge("create_file", "execute_multi_commands")
    workflow.add_edge("execute_multi_commands", "format_response")
    
    # MCPå·¥å…·è·¯å¾„
    workflow.add_edge("plan_mcp_tool", "execute_mcp_tool")
    workflow.add_edge("execute_mcp_tool", "format_response")
    
    # ç»“æŸèŠ‚ç‚¹
    workflow.add_edge("format_response", END)
    workflow.add_edge("answer_question", END)

    return workflow.compile()


# ============================================
# äº¤äº’å¼ä¸»å‡½æ•°
# ============================================

def print_header():
    """æ‰“å°æ¬¢è¿ä¿¡æ¯"""
    print("\n" + "=" * 80)
    print("ğŸ¤– AIæ™ºèƒ½ç»ˆç«¯åŠ©æ‰‹ - äº¤äº’å¼ç‰ˆæœ¬ + MCPé›†æˆ")
    print("=" * 80)
    print("\nâœ¨ åŠŸèƒ½:")
    print("  â€¢ è‡ªç„¶è¯­è¨€æ‰§è¡Œç»ˆç«¯å‘½ä»¤")
    print("  â€¢ åˆ›å»ºå’Œæ‰§è¡Œä»£ç æ–‡ä»¶")
    print("  â€¢ æ™ºèƒ½é—®ç­”")
    print("  â€¢ å¯¹è¯è®°å¿†ï¼ˆè®°ä½ä¸Šä¸‹æ–‡ï¼‰")
    print("\nğŸ”Œ MCPåŠŸèƒ½:")
    print("  â€¢ æ–‡ä»¶ç³»ç»Ÿ: è¯»å–/å†™å…¥/åˆ—å‡º/æœç´¢æ–‡ä»¶")
    print("  â€¢ æ¡Œé¢æ§åˆ¶: æˆªå›¾/å‰ªè´´æ¿/æ‰§è¡Œå‘½ä»¤")
    
    # æ˜¾ç¤ºå¯ç”¨å·¥å…·æ•°é‡
    tools = mcp_manager.list_available_tools()
    fs_tools = [t for t in tools if t['type'] == 'filesystem']
    desktop_tools = [t for t in tools if t['type'] == 'desktop-commander']
    print(f"  â€¢ å·²åŠ è½½: {len(fs_tools)}ä¸ªæ–‡ä»¶å·¥å…·, {len(desktop_tools)}ä¸ªæ¡Œé¢å·¥å…·")
    
    print("\nğŸ”§ åŒLLMé…ç½®:")
    print(f"  â€¢ é€šç”¨æ¨¡å‹: {LLM_CONFIG['model']} (æ„å›¾åˆ†æã€é—®ç­”)")
    print(f"  â€¢ ä»£ç æ¨¡å‹: {LLM_CONFIG2['model']} (å‘½ä»¤ç”Ÿæˆã€ä»£ç ç¼–å†™)")
    print("\nğŸ’¡ ç‰¹æ®Šå‘½ä»¤:")
    print("  â€¢ 'exit' æˆ– 'quit' - é€€å‡ºç¨‹åº")
    print("  â€¢ 'clear' - æ¸…ç©ºå¯¹è¯å†å²")
    print("  â€¢ 'history' - æŸ¥çœ‹å¯¹è¯å†å²")
    print("  â€¢ 'commands' - æŸ¥çœ‹å‘½ä»¤æ‰§è¡Œå†å²")
    print("  â€¢ 'models' - æŸ¥çœ‹å½“å‰æ¨¡å‹é…ç½®")
    print("  â€¢ 'tools' - æŸ¥çœ‹MCPå·¥å…·åˆ—è¡¨")
    print("\n" + "=" * 80 + "\n")


def handle_special_commands(user_input: str) -> bool:
    """å¤„ç†ç‰¹æ®Šå‘½ä»¤ï¼Œè¿”å›Trueè¡¨ç¤ºå·²å¤„ç†"""
    user_input_lower = user_input.lower().strip()
    
    if user_input_lower in ['exit', 'quit', 'é€€å‡º']:
        print("\nğŸ‘‹ å†è§ï¼æ„Ÿè°¢ä½¿ç”¨AIæ™ºèƒ½ç»ˆç«¯åŠ©æ‰‹ï¼\n")
        return True
    
    if user_input_lower in ['clear', 'æ¸…ç©º']:
        memory.clear()
        print("\nâœ… å¯¹è¯å†å²å·²æ¸…ç©º\n")
        return False
    
    if user_input_lower in ['history', 'å†å²']:
        if not memory.history:
            print("\næš‚æ— å¯¹è¯å†å²\n")
        else:
            print("\nğŸ“œ å¯¹è¯å†å²:")
            print("â”€" * 80)
            for idx, interaction in enumerate(memory.history, 1):
                print(f"\n[{interaction['timestamp']}]")
                print(f"ğŸ‘¤ ç”¨æˆ·: {interaction['user']}")
                print(f"ğŸ¤– åŠ©æ‰‹: {interaction['agent'][:200]}...")
                print(f"   (æ„å›¾: {interaction['intent']})")
            print("â”€" * 80 + "\n")
        return False
    
    if user_input_lower in ['commands', 'å‘½ä»¤']:
        if not memory.command_history:
            print("\næš‚æ— å‘½ä»¤æ‰§è¡Œå†å²\n")
        else:
            print("\nğŸ“‹ å‘½ä»¤æ‰§è¡Œå†å²:")
            print("â”€" * 80)
            for cmd in memory.command_history:
                status = "âœ…" if cmd["success"] else "âŒ"
                print(f"{status} [{cmd['timestamp']}] {cmd['command']}")
            print("â”€" * 80 + "\n")
        return False
    
    if user_input_lower in ['models', 'æ¨¡å‹']:
        print("\nğŸ”§ å½“å‰æ¨¡å‹é…ç½®:")
        print("â”€" * 80)
        print("\nğŸ“Œ é€šç”¨æ¨¡å‹ (LLM_CONFIG):")
        print(f"   æ¨¡å‹: {LLM_CONFIG['model']}")
        print(f"   API: {LLM_CONFIG['base_url']}")
        print(f"   ç”¨é€”: æ„å›¾åˆ†æã€æ™ºèƒ½é—®ç­”ã€ä¸Šä¸‹æ–‡ç†è§£")
        print(f"   ä½¿ç”¨åœºæ™¯: intent_analyzer(), question_answerer()")
        
        print("\nğŸ“Œ ä»£ç ç”Ÿæˆæ¨¡å‹ (LLM_CONFIG2):")
        print(f"   æ¨¡å‹: {LLM_CONFIG2['model']}")
        print(f"   API: {LLM_CONFIG2['base_url']}")
        print(f"   ç”¨é€”: å‘½ä»¤ç”Ÿæˆã€ä»£ç ç¼–å†™ã€ä»»åŠ¡è§„åˆ’")
        print(f"   ä½¿ç”¨åœºæ™¯: command_generator(), multi_step_planner(), mcp_tool_planner()")
        
        print("\nğŸ’¡ æç¤º:")
        print("   - ä¸åŒä»»åŠ¡ä½¿ç”¨æœ€é€‚åˆçš„æ¨¡å‹")
        print("   - ä»£ç ç”Ÿæˆä»»åŠ¡ä½¿ç”¨ä¸“ä¸šçš„ä»£ç æ¨¡å‹")
        print("   - å¯¹è¯å’Œç†è§£ä»»åŠ¡ä½¿ç”¨é€šç”¨æ¨¡å‹")
        print("â”€" * 80 + "\n")
        return False
    
    if user_input_lower in ['tools', 'å·¥å…·']:
        print("\nğŸ› ï¸ å¯ç”¨çš„MCPå·¥å…·:")
        print("â”€" * 80)
        tools = mcp_manager.list_available_tools()
        
        for tool_type in ['filesystem', 'desktop-commander']:
            type_tools = [t for t in tools if t['type'] == tool_type]
            if type_tools:
                icon = "ğŸ“" if tool_type == "filesystem" else "ğŸ–¥ï¸"
                print(f"\n{icon} {tool_type} ({len(type_tools)}ä¸ª):")
                for t in type_tools:
                    params_str = ", ".join(t['params'][:3])
                    if len(t['params']) > 3:
                        params_str += "..."
                    print(f"   â€¢ {t['name']:25} - {t['description']}")
                    print(f"     å‚æ•°: {params_str}")
        
        print("\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
        print("   â€¢ 'è¯»å–README.mdæ–‡ä»¶'")
        print("   â€¢ 'åˆ—å‡ºå½“å‰ç›®å½•çš„æ‰€æœ‰Pythonæ–‡ä»¶'")
        print("   â€¢ 'æœç´¢åŒ…å«LLM_CONFIGçš„æ–‡ä»¶'")
        print("   â€¢ 'å†™å…¥å†…å®¹åˆ°test.txtæ–‡ä»¶'")
        print("â”€" * 80 + "\n")
        return False
    
    return None


def main():
    """äº¤äº’å¼ä¸»å¾ªç¯"""
    
    print_header()
    
    agent = build_agent()
    
    print("ğŸ¬ å‡†å¤‡å°±ç»ªï¼è¯·è¾“å…¥ä½ çš„æŒ‡ä»¤æˆ–é—®é¢˜...\n")
    
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("ğŸ‘¤ ä½ : ").strip()
            
            if not user_input:
                continue
            
            # å¤„ç†ç‰¹æ®Šå‘½ä»¤
            special_result = handle_special_commands(user_input)
            if special_result is True:  # é€€å‡º
                break
            elif special_result is False:  # å·²å¤„ç†ï¼Œç»§ç»­å¾ªç¯
                continue
            
            print()  # ç©ºè¡Œ
            
            # æ„å»ºåˆå§‹çŠ¶æ€
            initial_state: AgentState = {
                "user_input": user_input,
                "intent": "unknown",
                "command": "",
                "commands": [],
                "command_output": "",
                "command_outputs": [],
                "response": "",
                "error": "",
                "needs_file_creation": False,
                "file_path": "",
                "file_content": "",
                "chat_history": [],
                # MCPç›¸å…³å­—æ®µ
                "mcp_tool": "",
                "mcp_params": {},
                "mcp_result": ""
            }
            
            # æ‰§è¡Œå·¥ä½œæµ
            result = agent.invoke(initial_state)
            
            # æ˜¾ç¤ºå“åº”
            print("â”€" * 80)
            print(f"ğŸ¤– åŠ©æ‰‹: {result['response']}")
            print("â”€" * 80 + "\n")
            
            # ä¿å­˜åˆ°è®°å¿†
            memory.add_interaction(
                user_input, 
                result['response'], 
                result.get('intent', 'unknown')
            )
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œé€€å‡ºç¨‹åº...\n")
            break
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}\n")
            print("è¯·é‡è¯•æˆ–è¾“å…¥ 'exit' é€€å‡º\n")


if __name__ == "__main__":
    main()
