"""
é—®é¢˜å›ç­”èŠ‚ç‚¹
æä¾›æµå¼æ‰“å­—æœºæ•ˆæœçš„é—®ç­”åŠŸèƒ½
"""

import time
import threading
from queue import Queue
from langchain_core.messages import HumanMessage

from src.core.agent_config import AgentState, LLM_CONFIG
from src.core.agent_memory import memory
from src.core.agent_llm import llm


def question_answerer(state: AgentState) -> dict:
    """å›ç­”ç”¨æˆ·é—®é¢˜ï¼ˆæ‰“å­—æœºæ•ˆæœæµå¼è¾“å‡ºï¼‰"""
    user_input = state["user_input"]
    context = memory.get_context_string()
    recent_commands = memory.get_recent_commands()

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIç»ˆç«¯åŠ©æ‰‹ã€‚å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¯¹è¯å†å²æä¾›æ›´å¥½çš„å¸®åŠ©ã€‚

{context}

{recent_commands}

å½“å‰é—®é¢˜: {user_input}

è¯·ç®€æ´ä½†å…¨é¢åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœç”¨æˆ·æåˆ°\"åˆšæ‰\"ã€\"ä¹‹å‰\"ç­‰è¯ï¼Œè¯·å‚è€ƒå¯¹è¯å†å²ã€‚

å›ç­”:"""

    print(f"[é—®é¢˜å›ç­”] ç”Ÿæˆå›ç­”")
    print(f"           ä½¿ç”¨æ¨¡å‹: {LLM_CONFIG['model']}")
    print()  # ç©ºè¡Œ
    print("â”€" * 80)
    print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)

    # æ‰“å­—æœºæ•ˆæœæµå¼è¾“å‡º
    try:
        response = ""
        char_queue = Queue()
        output_finished = threading.Event()

        def typewriter_output():
            """æ‰“å­—æœºè¾“å‡ºçº¿ç¨‹"""
            while not output_finished.is_set() or not char_queue.empty():
                try:
                    # ä»é˜Ÿåˆ—è·å–å­—ç¬¦ï¼Œè¶…æ—¶é¿å…æ­»é”
                    char = char_queue.get(timeout=0.1)
                    print(char, end="", flush=True)

                    # æ™ºèƒ½æ‰“å­—æœºå»¶è¿Ÿ
                    if char in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š':  # æ ‡ç‚¹ç¬¦å·ç¨å¾®åœé¡¿
                        time.sleep(0.06)
                    elif char == ' ':  # ç©ºæ ¼å¿«é€Ÿè·³è¿‡
                        time.sleep(0.01)
                    else:  # æ™®é€šå­—ç¬¦
                        time.sleep(0.03)

                except:
                    continue

        # å¯åŠ¨æ‰“å­—æœºè¾“å‡ºçº¿ç¨‹
        output_thread = threading.Thread(target=typewriter_output, daemon=True)
        output_thread.start()

        # æ”¶é›†LLMè¾“å‡ºå¹¶é€å­—ç¬¦æ”¾å…¥é˜Ÿåˆ—
        for chunk in llm.stream([HumanMessage(content=prompt)]):
            if hasattr(chunk, "content") and chunk.content:
                content = chunk.content
                response += content

                # é€å­—ç¬¦æ”¾å…¥é˜Ÿåˆ—
                for char in content:
                    char_queue.put(char)

        # æ ‡è®°è¾“å‡ºå®Œæˆ
        output_finished.set()

        # ç­‰å¾…è¾“å‡ºçº¿ç¨‹å®Œæˆ
        output_thread.join(timeout=5.0)  # æœ€å¤šç­‰å¾…5ç§’

        # ç¡®ä¿æ‰€æœ‰å­—ç¬¦éƒ½è¾“å‡ºå®Œæ¯•
        while not char_queue.empty():
            try:
                char = char_queue.get_nowait()
                print(char, end="", flush=True)
            except:
                break

        print()  # æ¢è¡Œ
        print("â”€" * 80)

        return {"response": response}

    except Exception as e:
        error_msg = f"âŒ ç”Ÿæˆå›ç­”å¤±è´¥: {str(e)}"
        print(error_msg)
        print("â”€" * 80)
        return {"response": error_msg, "error": str(e)}
