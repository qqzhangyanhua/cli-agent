"""
å¢å¼ºçš„ LLM åˆå§‹åŒ–æ¨¡å—
é›†æˆé”™è¯¯å¤„ç†ã€æ€§èƒ½ç›‘æ§å’Œé™çº§ç­–ç•¥
"""

from typing import List, Optional, Dict, Any
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage

from src.core.agent_config import LLM_CONFIG, LLM_CONFIG2, DEFAULT_HEADERS
from src.core.agent_metrics import get_metrics_collector
from src.core.agent_error_handler import get_llm_fallback_handler, LLMType, LLMCallResult


class EnhancedLLM:
    """å¢å¼ºçš„ LLM åŒ…è£…å™¨ï¼Œé›†æˆç›‘æ§å’Œé”™è¯¯å¤„ç†"""
    
    def __init__(self, config: Dict[str, Any], llm_type: LLMType, name: str):
        self.config = config
        self.llm_type = llm_type
        self.name = name
        self.metrics = get_metrics_collector()
        self.fallback_handler = get_llm_fallback_handler()
        
        # åˆ›å»ºåŸå§‹ LLM å®ä¾‹
        self._base_llm = ChatOpenAI(
            model=config["model"],
            base_url=config["base_url"],
            api_key=config["api_key"],
            temperature=config["temperature"],
            default_headers=DEFAULT_HEADERS
        )
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.call_count = 0
        self.success_count = 0
        self.total_tokens = {"prompt": 0, "completion": 0, "total": 0}
    
    def invoke(self, messages: List[BaseMessage], context_type: str = "default", 
              max_retries: int = 3) -> Any:
        """
        å¢å¼ºçš„ LLM è°ƒç”¨æ–¹æ³•
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            context_type: ä¸Šä¸‹æ–‡ç±»å‹ (question, command_generation, multi_step_planning)
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            LLM å“åº”ç»“æœ
        """
        self.call_count += 1
        
        # ä½¿ç”¨é™çº§å¤„ç†å™¨è°ƒç”¨ LLM
        result: LLMCallResult = self.fallback_handler.call_llm_with_fallback(
            messages=messages,
            llm_type=self.llm_type,
            context_type=context_type,
            max_retries=max_retries
        )
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        if result.success:
            self.success_count += 1
            
            # æ›´æ–° Token ç»Ÿè®¡
            if result.token_usage:
                for key, value in result.token_usage.items():
                    if key == "prompt_tokens":
                        self.total_tokens["prompt"] += value
                    elif key == "completion_tokens":
                        self.total_tokens["completion"] += value
                    elif key == "total_tokens":
                        self.total_tokens["total"] += value
        
        # åˆ›å»ºå…¼å®¹çš„å“åº”å¯¹è±¡
        class LLMResponse:
            def __init__(self, content: str, token_usage: Optional[Dict] = None):
                self.content = content
                self.usage_metadata = token_usage or {}
                self.response_metadata = {"token_usage": token_usage or {}}
        
        return LLMResponse(result.content, result.token_usage)
    
    def stream(self, messages: List[BaseMessage], context_type: str = "question", 
               max_retries: int = 3):
        """
        æµå¼è°ƒç”¨æ–¹æ³•ï¼ˆç”¨äºæ‰“å­—æœºæ•ˆæœï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            context_type: ä¸Šä¸‹æ–‡ç±»å‹
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Yields:
            æµå¼å“åº”å—
        """
        self.call_count += 1
        
        # å°è¯•ä½¿ç”¨åŸå§‹ LLM è¿›è¡Œæµå¼è°ƒç”¨
        try:
            with self.metrics.measure_operation("llm_stream", self.model_name) as ctx:
                total_content = ""
                
                for chunk in self._base_llm.stream(messages):
                    if hasattr(chunk, "content") and chunk.content:
                        total_content += chunk.content
                    yield chunk
                
                # è®°å½•æˆåŠŸçš„æµå¼è°ƒç”¨
                self.success_count += 1
                ctx["additional_data"] = {"stream_mode": True, "content_length": len(total_content)}
                
        except Exception as e:
            print(f"ğŸš¨ æµå¼è°ƒç”¨å¤±è´¥: {self.model_name} - {str(e)}")
            
            # æµå¼è°ƒç”¨å¤±è´¥æ—¶ï¼Œé™çº§åˆ°æ™®é€šè°ƒç”¨å¹¶æ¨¡æ‹Ÿæµå¼è¾“å‡º
            try:
                result: LLMCallResult = self.fallback_handler.call_llm_with_fallback(
                    messages=messages,
                    llm_type=self.llm_type,
                    context_type=context_type,
                    max_retries=max_retries
                )
                
                if result.success:
                    # æ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼šå°†å®Œæ•´å“åº”åˆ†å—è¿”å›
                    content = result.content
                    chunk_size = 5  # æ¯æ¬¡è¿”å›5ä¸ªå­—ç¬¦
                    
                    for i in range(0, len(content), chunk_size):
                        chunk_content = content[i:i + chunk_size]
                        
                        # åˆ›å»ºæ¨¡æ‹Ÿçš„æµå¼å“åº”å—
                        class StreamChunk:
                            def __init__(self, content: str):
                                self.content = content
                        
                        yield StreamChunk(chunk_content)
                else:
                    # å¦‚æœé™çº§ä¹Ÿå¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
                    class StreamChunk:
                        def __init__(self, content: str):
                            self.content = content
                    
                    yield StreamChunk(result.content)
                    
            except Exception as fallback_error:
                print(f"ğŸš¨ é™çº§æµå¼è°ƒç”¨ä¹Ÿå¤±è´¥: {str(fallback_error)}")
                
                # æœ€ç»ˆé™çº§ï¼šè¿”å›é”™è¯¯ä¿¡æ¯
                class StreamChunk:
                    def __init__(self, content: str):
                        self.content = content
                
                yield StreamChunk("æŠ±æ­‰ï¼ŒAI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å– LLM ç»Ÿè®¡ä¿¡æ¯"""
        success_rate = self.success_count / max(self.call_count, 1)
        
        return {
            "name": self.name,
            "model": self.config["model"],
            "call_count": self.call_count,
            "success_count": self.success_count,
            "success_rate": success_rate,
            "total_tokens": self.total_tokens.copy()
        }
    
    @property
    def model_name(self) -> str:
        """è·å–æ¨¡å‹åç§°"""
        return self.config["model"]


# åˆ›å»ºå¢å¼ºçš„ LLM å®ä¾‹
llm = EnhancedLLM(LLM_CONFIG, LLMType.PRIMARY, "é€šç”¨LLM")
llm_code = EnhancedLLM(LLM_CONFIG2, LLMType.SECONDARY, "ä»£ç LLM")


def get_llm_stats() -> Dict[str, Any]:
    """è·å–æ‰€æœ‰ LLM çš„ç»Ÿè®¡ä¿¡æ¯"""
    return {
        "primary_llm": llm.get_stats(),
        "secondary_llm": llm_code.get_stats(),
        "session_summary": {
            "total_calls": llm.call_count + llm_code.call_count,
            "total_tokens": {
                "prompt": llm.total_tokens["prompt"] + llm_code.total_tokens["prompt"],
                "completion": llm.total_tokens["completion"] + llm_code.total_tokens["completion"],
                "total": llm.total_tokens["total"] + llm_code.total_tokens["total"]
            }
        }
    }


def reset_llm_stats():
    """é‡ç½® LLM ç»Ÿè®¡ä¿¡æ¯"""
    llm.call_count = 0
    llm.success_count = 0
    llm.total_tokens = {"prompt": 0, "completion": 0, "total": 0}
    
    llm_code.call_count = 0
    llm_code.success_count = 0
    llm_code.total_tokens = {"prompt": 0, "completion": 0, "total": 0}
