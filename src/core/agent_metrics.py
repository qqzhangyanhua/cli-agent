"""
æ€§èƒ½æŒ‡æ ‡æ”¶é›†å’Œç›‘æ§æ¨¡å—
ç”¨äºæ”¶é›† LLM è°ƒç”¨ã€å·¥å…·æ‰§è¡Œã€å‘½ä»¤æ‰§è¡Œç­‰æ€§èƒ½æ•°æ®
"""

import time
import json
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, ContextManager
from dataclasses import dataclass, field, asdict
from contextlib import contextmanager
from pathlib import Path


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç»“æ„"""
    timestamp: datetime
    operation_type: str  # 'llm_call', 'tool_call', 'command_exec', 'file_op'
    operation_name: str
    duration_ms: float
    success: bool = True
    error_message: Optional[str] = None
    token_usage: Optional[Dict[str, int]] = None
    additional_data: Optional[Dict[str, Any]] = None


@dataclass
class SessionStats:
    """ä¼šè¯ç»Ÿè®¡ä¿¡æ¯"""
    start_time: datetime = field(default_factory=datetime.now)
    total_operations: int = 0
    successful_operations: int = 0
    failed_operations: int = 0
    total_duration_ms: float = 0.0
    total_tokens: Dict[str, int] = field(default_factory=lambda: {
        "prompt_tokens": 0,
        "completion_tokens": 0,
        "total_tokens": 0
    })
    llm_calls: int = 0
    tool_calls: int = 0
    command_executions: int = 0
    
    @property
    def success_rate(self) -> float:
        """æˆåŠŸç‡"""
        if self.total_operations == 0:
            return 0.0
        return self.successful_operations / self.total_operations
    
    @property
    def average_duration_ms(self) -> float:
        """å¹³å‡æ‰§è¡Œæ—¶é—´"""
        if self.total_operations == 0:
            return 0.0
        return self.total_duration_ms / self.total_operations
    
    @property
    def session_duration_minutes(self) -> float:
        """ä¼šè¯æŒç»­æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰"""
        return (datetime.now() - self.start_time).total_seconds() / 60


class MetricsCollector:
    """æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self, buffer_size: int = 1000, auto_export: bool = True):
        self.buffer_size = buffer_size
        self.auto_export = auto_export
        self.metrics_buffer: List[PerformanceMetrics] = []
        self.session_stats = SessionStats()
        self._lock = threading.Lock()
        
        # å¯¼å‡ºé…ç½®
        self.export_file = Path("performance_metrics.json")
        self.last_export_time = datetime.now()
        self.export_interval = timedelta(hours=1)  # æ¯å°æ—¶å¯¼å‡ºä¸€æ¬¡
    
    @contextmanager
    def measure_operation(self, op_type: str, op_name: str, **kwargs) -> ContextManager[Dict]:
        """
        æ€§èƒ½æµ‹é‡ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        
        Args:
            op_type: æ“ä½œç±»å‹ ('llm_call', 'tool_call', 'command_exec', 'file_op')
            op_name: æ“ä½œåç§°
            **kwargs: é¢å¤–æ•°æ®
        
        Usage:
            with metrics.measure_operation("llm_call", "kimi-k2") as ctx:
                result = llm.invoke(messages)
                ctx["token_usage"] = extract_token_usage(result)
        """
        start_time = time.time()
        context = {"additional_data": kwargs}
        
        try:
            yield context
            # æˆåŠŸå®Œæˆ
            duration = (time.time() - start_time) * 1000
            self._record_metric(
                op_type=op_type,
                op_name=op_name,
                duration_ms=duration,
                success=True,
                token_usage=context.get("token_usage"),
                additional_data=context.get("additional_data")
            )
        except Exception as e:
            # æ‰§è¡Œå¤±è´¥
            duration = (time.time() - start_time) * 1000
            self._record_metric(
                op_type=op_type,
                op_name=op_name,
                duration_ms=duration,
                success=False,
                error_message=str(e),
                additional_data=context.get("additional_data")
            )
            raise
    
    def _record_metric(self, op_type: str, op_name: str, duration_ms: float, 
                      success: bool, error_message: Optional[str] = None,
                      token_usage: Optional[Dict[str, int]] = None,
                      additional_data: Optional[Dict[str, Any]] = None):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        metric = PerformanceMetrics(
            timestamp=datetime.now(),
            operation_type=op_type,
            operation_name=op_name,
            duration_ms=duration_ms,
            success=success,
            error_message=error_message,
            token_usage=token_usage,
            additional_data=additional_data
        )
        
        with self._lock:
            # æ·»åŠ åˆ°ç¼“å†²åŒº
            self.metrics_buffer.append(metric)
            
            # æ›´æ–°ä¼šè¯ç»Ÿè®¡
            self._update_session_stats(metric)
            
            # æ£€æŸ¥ç¼“å†²åŒºå¤§å°
            if len(self.metrics_buffer) > self.buffer_size:
                self.metrics_buffer = self.metrics_buffer[-self.buffer_size:]
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¯¼å‡º
            if self.auto_export and self._should_export():
                self._export_metrics()
    
    def _update_session_stats(self, metric: PerformanceMetrics):
        """æ›´æ–°ä¼šè¯ç»Ÿè®¡ä¿¡æ¯"""
        self.session_stats.total_operations += 1
        self.session_stats.total_duration_ms += metric.duration_ms
        
        if metric.success:
            self.session_stats.successful_operations += 1
        else:
            self.session_stats.failed_operations += 1
        
        # æŒ‰æ“ä½œç±»å‹ç»Ÿè®¡
        if metric.operation_type == "llm_call":
            self.session_stats.llm_calls += 1
            # æ›´æ–° Token ç»Ÿè®¡
            if metric.token_usage:
                for key, value in metric.token_usage.items():
                    if key in self.session_stats.total_tokens:
                        self.session_stats.total_tokens[key] += value
        elif metric.operation_type == "tool_call":
            self.session_stats.tool_calls += 1
        elif metric.operation_type == "command_exec":
            self.session_stats.command_executions += 1
    
    def _should_export(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥å¯¼å‡ºæŒ‡æ ‡"""
        return datetime.now() - self.last_export_time > self.export_interval
    
    def _export_metrics(self):
        """å¯¼å‡ºæŒ‡æ ‡åˆ°æ–‡ä»¶"""
        try:
            export_data = {
                "export_time": datetime.now().isoformat(),
                "session_stats": asdict(self.session_stats),
                "recent_metrics": [asdict(m) for m in self.metrics_buffer[-100:]]  # æœ€è¿‘100æ¡
            }
            
            with open(self.export_file, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2, default=str)
            
            self.last_export_time = datetime.now()
        except Exception as e:
            print(f"âš ï¸ å¯¼å‡ºæ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
    
    def get_session_stats(self) -> SessionStats:
        """è·å–å½“å‰ä¼šè¯ç»Ÿè®¡"""
        with self._lock:
            return self.session_stats
    
    def get_recent_metrics(self, count: int = 50) -> List[PerformanceMetrics]:
        """è·å–æœ€è¿‘çš„æ€§èƒ½æŒ‡æ ‡"""
        with self._lock:
            return self.metrics_buffer[-count:] if self.metrics_buffer else []
    
    def get_operation_stats(self, op_type: Optional[str] = None) -> Dict[str, Any]:
        """è·å–æ“ä½œç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            metrics = self.metrics_buffer
            if op_type:
                metrics = [m for m in metrics if m.operation_type == op_type]
            
            if not metrics:
                return {"count": 0, "success_rate": 0.0, "avg_duration_ms": 0.0}
            
            successful = sum(1 for m in metrics if m.success)
            total_duration = sum(m.duration_ms for m in metrics)
            
            return {
                "count": len(metrics),
                "success_rate": successful / len(metrics),
                "avg_duration_ms": total_duration / len(metrics),
                "total_duration_ms": total_duration,
                "successful_operations": successful,
                "failed_operations": len(metrics) - successful
            }
    
    def get_token_usage_summary(self) -> Dict[str, int]:
        """è·å– Token ä½¿ç”¨æ±‡æ€»"""
        with self._lock:
            return self.session_stats.total_tokens.copy()
    
    def reset_session_stats(self):
        """é‡ç½®ä¼šè¯ç»Ÿè®¡"""
        with self._lock:
            self.session_stats = SessionStats()
            self.metrics_buffer.clear()
    
    def format_stats_report(self) -> str:
        """æ ¼å¼åŒ–ç»Ÿè®¡æŠ¥å‘Š"""
        stats = self.get_session_stats()
        
        report = f"""
ğŸ“Š æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š
{'=' * 50}

â±ï¸  ä¼šè¯ä¿¡æ¯:
   æŒç»­æ—¶é—´: {stats.session_duration_minutes:.1f} åˆ†é’Ÿ
   å¼€å§‹æ—¶é—´: {stats.start_time.strftime('%Y-%m-%d %H:%M:%S')}

ğŸ¯ æ“ä½œç»Ÿè®¡:
   æ€»æ“ä½œæ•°: {stats.total_operations}
   æˆåŠŸç‡: {stats.success_rate:.1%}
   å¹³å‡è€—æ—¶: {stats.average_duration_ms:.1f}ms

ğŸ”§ æ“ä½œåˆ†ç±»:
   LLM è°ƒç”¨: {stats.llm_calls}
   å·¥å…·è°ƒç”¨: {stats.tool_calls}
   å‘½ä»¤æ‰§è¡Œ: {stats.command_executions}

ğŸª™ Token ä½¿ç”¨:
   è¾“å…¥ Token: {stats.total_tokens['prompt_tokens']:,}
   è¾“å‡º Token: {stats.total_tokens['completion_tokens']:,}
   æ€»è®¡ Token: {stats.total_tokens['total_tokens']:,}

ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:
   æ€»è€—æ—¶: {stats.total_duration_ms:.1f}ms
   æˆåŠŸæ“ä½œ: {stats.successful_operations}
   å¤±è´¥æ“ä½œ: {stats.failed_operations}
"""
        return report


# å…¨å±€æŒ‡æ ‡æ”¶é›†å™¨å®ä¾‹
metrics_collector = MetricsCollector()


def get_metrics_collector() -> MetricsCollector:
    """è·å–å…¨å±€æŒ‡æ ‡æ”¶é›†å™¨"""
    return metrics_collector


# ä¾¿æ·è£…é¥°å™¨
def measure_performance(op_type: str, op_name: str = None):
    """æ€§èƒ½æµ‹é‡è£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            name = op_name or func.__name__
            with metrics_collector.measure_operation(op_type, name):
                return func(*args, **kwargs)
        return wrapper
    return decorator
