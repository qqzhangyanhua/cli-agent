"""
LLM é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥æ¨¡å—
ä¸“é—¨å¤„ç† LLM è°ƒç”¨å¤±è´¥çš„å„ç§æƒ…å†µ
"""

import time
import random
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from enum import Enum

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage

from src.core.agent_config import LLM_CONFIG, LLM_CONFIG2, DEFAULT_HEADERS
from src.core.agent_resilience import ErrorContext, ErrorType, FallbackResult, FallbackStrategy
from src.core.agent_metrics import get_metrics_collector


class LLMType(Enum):
    """LLM ç±»å‹æšä¸¾"""
    PRIMARY = "primary"    # ä¸»è¦æ¨¡å‹ (Kimi)
    SECONDARY = "secondary"  # å¤‡ç”¨æ¨¡å‹ (Claude)


@dataclass
class LLMCallResult:
    """LLM è°ƒç”¨ç»“æœ"""
    success: bool
    content: str
    model_used: str
    token_usage: Optional[Dict[str, int]] = None
    error_message: Optional[str] = None
    fallback_used: bool = False
    strategy_used: Optional[FallbackStrategy] = None


class LLMFallbackHandler:
    """LLM é™çº§å¤„ç†å™¨"""
    
    def __init__(self):
        self.metrics = get_metrics_collector()
        
        # åˆå§‹åŒ–ä¸¤ä¸ª LLM å®ä¾‹
        self.primary_llm = ChatOpenAI(
            model=LLM_CONFIG["model"],
            base_url=LLM_CONFIG["base_url"],
            api_key=LLM_CONFIG["api_key"],
            temperature=LLM_CONFIG["temperature"],
            default_headers=DEFAULT_HEADERS
        )
        
        self.secondary_llm = ChatOpenAI(
            model=LLM_CONFIG2["model"],
            base_url=LLM_CONFIG2["base_url"],
            api_key=LLM_CONFIG2["api_key"],
            temperature=LLM_CONFIG2["temperature"],
            default_headers=DEFAULT_HEADERS
        )
        
        # æ¨¡æ¿å“åº”åº“
        self.response_templates = {
            "question": "æŠ±æ­‰ï¼ŒAI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚æˆ‘å·²è®°å½•æ‚¨çš„é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚",
            "command_generation": "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå‘½ä»¤ã€‚è¯·æ‰‹åŠ¨æ‰§è¡Œç›¸å…³æ“ä½œã€‚",
            "multi_step_planning": "æŠ±æ­‰ï¼Œæ— æ³•åˆ¶å®šæ‰§è¡Œè®¡åˆ’ã€‚è¯·å°†ä»»åŠ¡åˆ†è§£ä¸ºæ›´ç®€å•çš„æ­¥éª¤ã€‚",
            "default": "æŠ±æ­‰ï¼ŒAI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚"
        }
        
        # é™çº§ç­–ç•¥é…ç½®
        self.fallback_strategies = [
            self._retry_with_exponential_backoff,
            self._switch_to_backup_model,
            self._use_simplified_prompt,
            self._use_template_response
        ]
    
    def call_llm_with_fallback(self, messages: List, llm_type: LLMType = LLMType.PRIMARY, 
                              context_type: str = "default", max_retries: int = 3) -> LLMCallResult:
        """
        å¸¦é™çº§ç­–ç•¥çš„ LLM è°ƒç”¨
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            llm_type: LLM ç±»å‹
            context_type: ä¸Šä¸‹æ–‡ç±»å‹ (question, command_generation, multi_step_planning)
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            LLM è°ƒç”¨ç»“æœ
        """
        # é€‰æ‹©åˆå§‹ LLM
        current_llm = self.primary_llm if llm_type == LLMType.PRIMARY else self.secondary_llm
        model_name = LLM_CONFIG["model"] if llm_type == LLMType.PRIMARY else LLM_CONFIG2["model"]
        
        # åˆ›å»ºé”™è¯¯ä¸Šä¸‹æ–‡
        error_context = ErrorContext(
            error_type=ErrorType.LLM_CALL_FAILED,
            error_message="",
            node_name="llm_call",
            user_input=str(messages),
            operation_name="llm_call"
        )
        
        # å°è¯•ç›´æ¥è°ƒç”¨
        with self.metrics.measure_operation("llm_call", model_name) as ctx:
            try:
                result = current_llm.invoke(messages)
                
                # æå– Token ä½¿ç”¨ä¿¡æ¯
                token_usage = self._extract_token_usage(result)
                ctx["token_usage"] = token_usage
                
                return LLMCallResult(
                    success=True,
                    content=result.content,
                    model_used=model_name,
                    token_usage=token_usage
                )
                
            except Exception as e:
                error_context.error_message = str(e)
                print(f"ğŸš¨ LLM è°ƒç”¨å¤±è´¥: {model_name} - {str(e)}")
                
                # æ‰§è¡Œé™çº§ç­–ç•¥
                return self._execute_fallback_strategies(
                    messages, error_context, context_type, max_retries
                )
    
    def _execute_fallback_strategies(self, messages: List, error_context: ErrorContext, 
                                   context_type: str, max_retries: int) -> LLMCallResult:
        """æ‰§è¡Œé™çº§ç­–ç•¥é“¾"""
        
        for i, strategy in enumerate(self.fallback_strategies):
            try:
                print(f"ğŸ”„ å°è¯•é™çº§ç­–ç•¥ {i+1}: {strategy.__name__}")
                
                result = strategy(messages, error_context, context_type, max_retries)
                
                if result.success:
                    print(f"âœ… é™çº§ç­–ç•¥æˆåŠŸ: {strategy.__name__}")
                    result.fallback_used = True
                    return result
                else:
                    print(f"âŒ é™çº§ç­–ç•¥å¤±è´¥: {strategy.__name__} - {result.error_message}")
                    
            except Exception as e:
                print(f"âš ï¸ é™çº§ç­–ç•¥å¼‚å¸¸: {strategy.__name__} - {str(e)}")
                continue
        
        # æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œè¿”å›æœ€ç»ˆé™çº§
        return self._final_fallback(error_context, context_type)
    
    def _retry_with_exponential_backoff(self, messages: List, error_context: ErrorContext, 
                                      context_type: str, max_retries: int) -> LLMCallResult:
        """æŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥"""
        
        for attempt in range(max_retries):
            if attempt > 0:
                # è®¡ç®—å»¶è¿Ÿæ—¶é—´
                delay = min(2 ** attempt + random.uniform(0, 1), 30)  # æœ€å¤§30ç§’
                print(f"â±ï¸ é‡è¯•å»¶è¿Ÿ: {delay:.1f}s (ç¬¬ {attempt + 1} æ¬¡)")
                time.sleep(delay)
            
            try:
                # é‡æ–°å°è¯•åŸå§‹ LLM
                with self.metrics.measure_operation("llm_call_retry", "retry") as ctx:
                    result = self.primary_llm.invoke(messages)
                    token_usage = self._extract_token_usage(result)
                    ctx["token_usage"] = token_usage
                    
                    return LLMCallResult(
                        success=True,
                        content=result.content,
                        model_used=LLM_CONFIG["model"],
                        token_usage=token_usage,
                        strategy_used=FallbackStrategy.RETRY_WITH_BACKOFF
                    )
                    
            except Exception as e:
                print(f"âŒ é‡è¯•å¤±è´¥ (ç¬¬ {attempt + 1} æ¬¡): {str(e)}")
                if attempt == max_retries - 1:
                    return LLMCallResult(
                        success=False,
                        content="",
                        model_used=LLM_CONFIG["model"],
                        error_message=f"é‡è¯• {max_retries} æ¬¡åä»ç„¶å¤±è´¥: {str(e)}"
                    )
        
        return LLMCallResult(success=False, content="", model_used="", error_message="é‡è¯•å¤±è´¥")
    
    def _switch_to_backup_model(self, messages: List, error_context: ErrorContext, 
                              context_type: str, max_retries: int) -> LLMCallResult:
        """åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹ç­–ç•¥"""
        
        try:
            print(f"ğŸ”„ åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹: {LLM_CONFIG2['model']}")
            
            with self.metrics.measure_operation("llm_call_backup", LLM_CONFIG2["model"]) as ctx:
                result = self.secondary_llm.invoke(messages)
                token_usage = self._extract_token_usage(result)
                ctx["token_usage"] = token_usage
                
                return LLMCallResult(
                    success=True,
                    content=result.content,
                    model_used=LLM_CONFIG2["model"],
                    token_usage=token_usage,
                    strategy_used=FallbackStrategy.SWITCH_MODEL
                )
                
        except Exception as e:
            return LLMCallResult(
                success=False,
                content="",
                model_used=LLM_CONFIG2["model"],
                error_message=f"å¤‡ç”¨æ¨¡å‹ä¹Ÿå¤±è´¥: {str(e)}"
            )
    
    def _use_simplified_prompt(self, messages: List, error_context: ErrorContext, 
                             context_type: str, max_retries: int) -> LLMCallResult:
        """ä½¿ç”¨ç®€åŒ–æç¤ºç­–ç•¥"""
        
        try:
            # ç®€åŒ–æ¶ˆæ¯å†…å®¹
            simplified_messages = self._simplify_messages(messages, context_type)
            
            print(f"ğŸ”„ ä½¿ç”¨ç®€åŒ–æç¤º (é•¿åº¦: {len(str(simplified_messages))})")
            
            with self.metrics.measure_operation("llm_call_simplified", "simplified") as ctx:
                # å…ˆå°è¯•å¤‡ç”¨æ¨¡å‹
                result = self.secondary_llm.invoke(simplified_messages)
                token_usage = self._extract_token_usage(result)
                ctx["token_usage"] = token_usage
                
                return LLMCallResult(
                    success=True,
                    content=result.content,
                    model_used=LLM_CONFIG2["model"],
                    token_usage=token_usage,
                    strategy_used=FallbackStrategy.USE_TEMPLATE
                )
                
        except Exception as e:
            return LLMCallResult(
                success=False,
                content="",
                model_used="simplified",
                error_message=f"ç®€åŒ–æç¤ºå¤±è´¥: {str(e)}"
            )
    
    def _use_template_response(self, messages: List, error_context: ErrorContext, 
                             context_type: str, max_retries: int) -> LLMCallResult:
        """ä½¿ç”¨æ¨¡æ¿å“åº”ç­–ç•¥"""
        
        template = self.response_templates.get(context_type, self.response_templates["default"])
        
        # å°è¯•æ ¹æ®ç”¨æˆ·è¾“å…¥å®šåˆ¶æ¨¡æ¿
        try:
            if messages and hasattr(messages[-1], 'content'):
                user_content = messages[-1].content.lower()
                
                if any(word in user_content for word in ["å‘½ä»¤", "æ‰§è¡Œ", "è¿è¡Œ"]):
                    template = self.response_templates.get("command_generation", template)
                elif any(word in user_content for word in ["è®¡åˆ’", "æ­¥éª¤", "å¦‚ä½•"]):
                    template = self.response_templates.get("multi_step_planning", template)
                elif any(word in user_content for word in ["ä»€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "æ€ä¹ˆ", "?"]):
                    template = self.response_templates.get("question", template)
        except:
            pass  # å¦‚æœå®šåˆ¶å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿
        
        return LLMCallResult(
            success=True,
            content=template,
            model_used="template",
            strategy_used=FallbackStrategy.USE_TEMPLATE
        )
    
    def _final_fallback(self, error_context: ErrorContext, context_type: str) -> LLMCallResult:
        """æœ€ç»ˆé™çº§ç­–ç•¥"""
        
        final_message = f"""
âš ï¸ AI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨

æ‰€æœ‰é™çº§ç­–ç•¥éƒ½å·²å°è¯•ï¼Œä½†ä»æ— æ³•æä¾›æœåŠ¡ã€‚

é”™è¯¯ä¿¡æ¯: {error_context.error_message}
æ—¶é—´: {error_context.timestamp.strftime('%Y-%m-%d %H:%M:%S')}

å»ºè®®:
â€¢ æ£€æŸ¥ç½‘ç»œè¿æ¥
â€¢ ç¨åé‡è¯•
â€¢ å¦‚é—®é¢˜æŒç»­ï¼Œè¯·è”ç³»ç®¡ç†å‘˜

æ‚¨çš„è¯·æ±‚å·²è¢«è®°å½•ï¼Œæˆ‘ä»¬ä¼šå°½å¿«å¤„ç†ã€‚
"""
        
        return LLMCallResult(
            success=True,
            content=final_message,
            model_used="fallback",
            strategy_used=FallbackStrategy.GRACEFUL_DEGRADATION
        )
    
    def _simplify_messages(self, messages: List, context_type: str) -> List:
        """ç®€åŒ–æ¶ˆæ¯å†…å®¹"""
        
        simplified = []
        
        for msg in messages:
            if hasattr(msg, 'content'):
                content = msg.content
                
                # æ ¹æ®ä¸Šä¸‹æ–‡ç±»å‹ç®€åŒ–
                if context_type == "command_generation":
                    # ä¿ç•™å…³é”®ä¿¡æ¯ï¼Œç§»é™¤è¯¦ç»†è¯´æ˜
                    content = self._extract_command_keywords(content)
                elif context_type == "question":
                    # ä¿ç•™é—®é¢˜æ ¸å¿ƒï¼Œç§»é™¤å†—ä½™ä¿¡æ¯
                    content = self._extract_question_core(content)
                elif context_type == "multi_step_planning":
                    # ä¿ç•™ä»»åŠ¡æè¿°ï¼Œç§»é™¤è¯¦ç»†è¦æ±‚
                    content = self._extract_task_description(content)
                
                # é™åˆ¶é•¿åº¦
                if len(content) > 500:
                    content = content[:500] + "..."
                
                # åˆ›å»ºç®€åŒ–çš„æ¶ˆæ¯
                if isinstance(msg, HumanMessage):
                    simplified.append(HumanMessage(content=content))
                elif isinstance(msg, AIMessage):
                    simplified.append(AIMessage(content=content))
        
        return simplified
    
    def _extract_command_keywords(self, content: str) -> str:
        """æå–å‘½ä»¤ç›¸å…³å…³é”®è¯"""
        keywords = ["åˆ—å‡º", "æ˜¾ç¤º", "æŸ¥çœ‹", "åˆ›å»º", "åˆ é™¤", "è¿è¡Œ", "æ‰§è¡Œ", "å®‰è£…", "å¯åŠ¨", "åœæ­¢"]
        
        lines = content.split('\n')
        relevant_lines = []
        
        for line in lines:
            if any(keyword in line for keyword in keywords):
                relevant_lines.append(line)
        
        return '\n'.join(relevant_lines) if relevant_lines else content[:200]
    
    def _extract_question_core(self, content: str) -> str:
        """æå–é—®é¢˜æ ¸å¿ƒ"""
        question_words = ["ä»€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "æ€ä¹ˆ", "å¦‚ä½•", "å“ªé‡Œ", "ä»€ä¹ˆæ—¶å€™", "?", "ï¼Ÿ"]
        
        sentences = content.split('ã€‚')
        question_sentences = []
        
        for sentence in sentences:
            if any(word in sentence for word in question_words):
                question_sentences.append(sentence)
        
        return 'ã€‚'.join(question_sentences) if question_sentences else content[:200]
    
    def _extract_task_description(self, content: str) -> str:
        """æå–ä»»åŠ¡æè¿°"""
        task_words = ["éœ€è¦", "æƒ³è¦", "å¸Œæœ›", "è®¡åˆ’", "å‡†å¤‡", "æ‰“ç®—"]
        
        lines = content.split('\n')
        task_lines = []
        
        for line in lines:
            if any(word in line for word in task_words):
                task_lines.append(line)
        
        return '\n'.join(task_lines) if task_lines else content[:200]
    
    def _extract_token_usage(self, result) -> Optional[Dict[str, int]]:
        """æå– Token ä½¿ç”¨ä¿¡æ¯"""
        try:
            if hasattr(result, 'usage_metadata') and result.usage_metadata:
                return {
                    "prompt_tokens": result.usage_metadata.get('input_tokens', 0),
                    "completion_tokens": result.usage_metadata.get('output_tokens', 0),
                    "total_tokens": result.usage_metadata.get('total_tokens', 0)
                }
            elif hasattr(result, 'response_metadata') and result.response_metadata:
                usage = result.response_metadata.get('token_usage', {})
                return {
                    "prompt_tokens": usage.get('prompt_tokens', 0),
                    "completion_tokens": usage.get('completion_tokens', 0),
                    "total_tokens": usage.get('total_tokens', 0)
                }
        except Exception as e:
            print(f"âš ï¸ æå– Token ä½¿ç”¨ä¿¡æ¯å¤±è´¥: {e}")
        
        return None
    
    def get_model_health_status(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹å¥åº·çŠ¶æ€"""
        return {
            "primary_model": {
                "name": LLM_CONFIG["model"],
                "base_url": LLM_CONFIG["base_url"],
                "status": "unknown"  # éœ€è¦å®é™…æµ‹è¯•
            },
            "secondary_model": {
                "name": LLM_CONFIG2["model"],
                "base_url": LLM_CONFIG2["base_url"],
                "status": "unknown"  # éœ€è¦å®é™…æµ‹è¯•
            }
        }


# å…¨å±€ LLM é™çº§å¤„ç†å™¨å®ä¾‹
llm_fallback_handler = LLMFallbackHandler()


def get_llm_fallback_handler() -> LLMFallbackHandler:
    """è·å–å…¨å±€ LLM é™çº§å¤„ç†å™¨"""
    return llm_fallback_handler










