#!/usr/bin/env python3
"""
å¢å¼ºåŠŸèƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•é”™è¯¯å¤„ç†ã€æ€§èƒ½ç›‘æ§å’Œé™çº§ç­–ç•¥
"""

import sys
import os
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®ç›®å½•åˆ°Pythonè·¯å¾„
SCRIPT_DIR = Path(__file__).parent.absolute()
if str(SCRIPT_DIR) not in sys.path:
    sys.path.insert(0, str(SCRIPT_DIR))

from src.core.agent_metrics import get_metrics_collector
from src.core.agent_monitoring import get_monitoring_dashboard
from src.core.agent_resilience import get_resilience_manager
from src.core.agent_error_handler import get_llm_fallback_handler, LLMType
from src.core.agent_llm import get_llm_stats, reset_llm_stats
from langchain_core.messages import HumanMessage


def test_metrics_collection():
    """æµ‹è¯•æ€§èƒ½æŒ‡æ ‡æ”¶é›†"""
    print("ğŸ§ª æµ‹è¯•æ€§èƒ½æŒ‡æ ‡æ”¶é›†...")
    
    metrics = get_metrics_collector()
    
    # æ¨¡æ‹Ÿä¸€äº›æ“ä½œ
    with metrics.measure_operation("test_operation", "test_func") as ctx:
        time.sleep(0.1)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
        ctx["additional_data"] = {"test": "data"}
    
    # æ¨¡æ‹Ÿå¤±è´¥æ“ä½œ
    try:
        with metrics.measure_operation("test_operation", "failing_func"):
            time.sleep(0.05)
            raise Exception("æ¨¡æ‹Ÿé”™è¯¯")
    except Exception:
        pass
    
    # æ£€æŸ¥ç»Ÿè®¡
    stats = metrics.get_session_stats()
    print(f"  âœ… æ€»æ“ä½œæ•°: {stats.total_operations}")
    print(f"  âœ… æˆåŠŸç‡: {stats.success_rate:.1%}")
    print(f"  âœ… å¹³å‡è€—æ—¶: {stats.average_duration_ms:.1f}ms")
    
    return True


def test_llm_fallback():
    """æµ‹è¯• LLM é™çº§ç­–ç•¥"""
    print("\nğŸ§ª æµ‹è¯• LLM é™çº§ç­–ç•¥...")
    
    handler = get_llm_fallback_handler()
    
    # æµ‹è¯•æ­£å¸¸è°ƒç”¨
    try:
        messages = [HumanMessage(content="ä½ å¥½")]
        result = handler.call_llm_with_fallback(
            messages=messages,
            llm_type=LLMType.PRIMARY,
            context_type="question"
        )
        
        print(f"  âœ… LLM è°ƒç”¨æˆåŠŸ: {result.success}")
        print(f"  âœ… ä½¿ç”¨æ¨¡å‹: {result.model_used}")
        print(f"  âœ… å“åº”é•¿åº¦: {len(result.content)} å­—ç¬¦")
        
        if result.token_usage:
            print(f"  âœ… Token ä½¿ç”¨: {result.token_usage}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ LLM æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_resilience_manager():
    """æµ‹è¯•éŸ§æ€§ç®¡ç†å™¨"""
    print("\nğŸ§ª æµ‹è¯•éŸ§æ€§ç®¡ç†å™¨...")
    
    resilience = get_resilience_manager()
    
    # è·å–å¥åº·çŠ¶æ€
    health = resilience.get_health_status()
    print(f"  âœ… æ€»é”™è¯¯æ•°: {health['total_errors']}")
    print(f"  âœ… æ¢å¤ç‡: {health['recovery_rate']:.1%}")
    print(f"  âœ… ç†”æ–­å™¨æ•°é‡: {len(health['circuit_breakers'])}")
    
    return True


def test_monitoring_dashboard():
    """æµ‹è¯•ç›‘æ§ä»ªè¡¨æ¿"""
    print("\nğŸ§ª æµ‹è¯•ç›‘æ§ä»ªè¡¨æ¿...")
    
    dashboard = get_monitoring_dashboard()
    
    # è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€
    health = dashboard.get_system_health()
    print(f"  âœ… æ•´ä½“çŠ¶æ€: {health.overall_status}")
    print(f"  âœ… æ€§èƒ½åˆ†æ•°: {health.performance_score:.1f}/100")
    print(f"  âœ… ç»„ä»¶æ•°é‡: {len(health.components)}")
    
    # ç”Ÿæˆå¿«é€Ÿç»Ÿè®¡
    quick_stats = dashboard.get_quick_stats()
    print("  âœ… å¿«é€Ÿç»Ÿè®¡:")
    for line in quick_stats.strip().split('\n'):
        print(f"    {line}")
    
    return True


def test_enhanced_llm():
    """æµ‹è¯•å¢å¼ºçš„ LLM åŒ…è£…å™¨"""
    print("\nğŸ§ª æµ‹è¯•å¢å¼ºçš„ LLM åŒ…è£…å™¨...")
    
    from src.core.agent_llm import llm, llm_code
    
    try:
        # æµ‹è¯•é€šç”¨ LLM
        messages = [HumanMessage(content="ç®€å•å›ç­”ï¼š1+1ç­‰äºå¤šå°‘ï¼Ÿ")]
        response = llm.invoke(messages, context_type="question")
        
        print(f"  âœ… é€šç”¨ LLM è°ƒç”¨æˆåŠŸ")
        print(f"  âœ… å“åº”: {response.content[:50]}...")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = get_llm_stats()
        print(f"  âœ… LLM ç»Ÿè®¡: {stats['primary_llm']['call_count']} æ¬¡è°ƒç”¨")
        
        return True
        
    except Exception as e:
        print(f"  âŒ å¢å¼º LLM æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_mcp_monitoring():
    """æµ‹è¯• MCP å·¥å…·ç›‘æ§"""
    print("\nğŸ§ª æµ‹è¯• MCP å·¥å…·ç›‘æ§...")
    
    try:
        from src.mcp.mcp_manager import mcp_manager
        
        # æµ‹è¯•å·¥å…·è°ƒç”¨ï¼ˆä½¿ç”¨å†…ç½®å·¥å…·ï¼‰
        result = mcp_manager.call_tool("read_file", file_path="README.md")
        
        print(f"  âœ… MCP å·¥å…·è°ƒç”¨: {'æˆåŠŸ' if result.get('success') else 'å¤±è´¥'}")
        
        # æ£€æŸ¥æŒ‡æ ‡æ”¶é›†
        metrics = get_metrics_collector()
        tool_stats = metrics.get_operation_stats("tool_call")
        print(f"  âœ… å·¥å…·è°ƒç”¨ç»Ÿè®¡: {tool_stats['count']} æ¬¡")
        
        return True
        
    except Exception as e:
        print(f"  âŒ MCP ç›‘æ§æµ‹è¯•å¤±è´¥: {e}")
        return False


def run_comprehensive_test():
    """è¿è¡Œç»¼åˆæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹å¢å¼ºåŠŸèƒ½ç»¼åˆæµ‹è¯•")
    print("=" * 60)
    
    tests = [
        ("æ€§èƒ½æŒ‡æ ‡æ”¶é›†", test_metrics_collection),
        ("LLM é™çº§ç­–ç•¥", test_llm_fallback),
        ("éŸ§æ€§ç®¡ç†å™¨", test_resilience_manager),
        ("ç›‘æ§ä»ªè¡¨æ¿", test_monitoring_dashboard),
        ("å¢å¼º LLM", test_enhanced_llm),
        ("MCP å·¥å…·ç›‘æ§", test_mcp_monitoring),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"  âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
            results.append((test_name, False))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
    
    passed = 0
    for test_name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"  {status} {test_name}")
        if success:
            passed += 1
    
    print(f"\nğŸ¯ æ€»ä½“ç»“æœ: {passed}/{len(results)} é¡¹æµ‹è¯•é€šè¿‡")
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    print("\nğŸ“ˆ æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡:")
    dashboard = get_monitoring_dashboard()
    print(dashboard.get_quick_stats())
    
    return passed == len(results)


if __name__ == "__main__":
    success = run_comprehensive_test()
    sys.exit(0 if success else 1)



